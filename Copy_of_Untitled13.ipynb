{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNYKYlKXgC37tFFOZzAGYGP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/petervinhchau/public/blob/main/Copy_of_Untitled13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfsqwuSbLJhe"
      },
      "outputs": [],
      "source": [
        "# Check PyTorch and CUDA environment\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Install required dependencies\n",
        "!pip install pytorch-lightning torchaudio transformers tensorboard\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Create symbolic link for the project if it does not already exist\n",
        "#!ln -s /content/drive/MyDrive/colab/ece247/proj/tmp/emg2qwerty-main /content/emg2qwerty-main\n",
        "\n",
        "!unzip -oq /content/drive/MyDrive/colab/ece247/proj/tmp/emg2qwerty-main.zip -d /content/\n",
        "\n",
        "# Change directory to your project code\n",
        "%cd /content/emg2qwerty-main/\n",
        "!ls -l /content/emg2qwerty-main/\n",
        "\n",
        "!ls -l /content/drive/MyDrive/colab/ece247/proj/tmp/data/\n",
        "\n",
        "%cd /content/emg2qwerty-main/"
      ],
      "metadata": {
        "id": "Rb4zYGyFSYRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/emg2qwerty-main/models/lstm_encoder.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "\n",
        "class LSTMEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Bidirectional LSTM encoder for EMG signals.\n",
        "\n",
        "    Processes input sequences of shape (batch, time, features) and returns\n",
        "    output sequences of shape (batch, time, output_dim), where output_dim is\n",
        "    hidden_size * 2 (if bidirectional) to match the expected classifier input.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_size=256, num_layers=2, bidirectional=True, dropout=0.2):\n",
        "        super(LSTMEncoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=bidirectional,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "        # Output dimension: hidden_size*2 if bidirectional; else hidden_size.\n",
        "        self.output_dim = hidden_size * 2 if bidirectional else hidden_size\n",
        "\n",
        "    def forward(self, x, lengths=None):\n",
        "        \"\"\"\n",
        "        Forward pass through the LSTM encoder.\n",
        "\n",
        "        Args:\n",
        "            x: Tensor of shape (batch_size, time_steps, input_dim)\n",
        "            lengths: Optional tensor with actual sequence lengths.\n",
        "\n",
        "        Returns:\n",
        "            lstm_out: Tensor of shape (batch_size, time_steps, output_dim)\n",
        "        \"\"\"\n",
        "        if lengths is not None:\n",
        "            # Pack the sequence for variable-length processing\n",
        "            x_packed = rnn_utils.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
        "            lstm_out, _ = self.lstm(x_packed)\n",
        "            lstm_out, _ = rnn_utils.pad_packed_sequence(lstm_out, batch_first=True)\n",
        "        else:\n",
        "            lstm_out, _ = self.lstm(x)\n",
        "        return lstm_out\n"
      ],
      "metadata": {
        "id": "PUwwDmgyVXj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Ensure the models directory exists\n",
        "if [ ! -d \"/content/emg2qwerty-main/models\" ]; then\n",
        "    mkdir -p emg2qwerty-main/models\n",
        "    echo \"Created directory: emg2qwerty-main/models\"\n",
        "fi\n",
        "\n",
        "# Ensure __init__.py exists in the models directory\n",
        "if [ ! -f \"/content/emg2qwerty-main/models/__init__.py\" ]; then\n",
        "    touch emg2qwerty-main/models/__init__.py\n",
        "    echo \"Created file: emg2qwerty-main/models/__init__.py\"\n",
        "fi\n",
        "\n",
        "# Append import for LSTMEncoder if not already present\n",
        "grep -q \"LSTMEncoder\" emg2qwerty-main/models/__init__.py || echo \"from .lstm_encoder import LSTMEncoder\" >> emg2qwerty-main/models/__init__.py\n"
      ],
      "metadata": {
        "id": "B0WGxrfLVv-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/emg2qwerty-main/test_encoders.py\n",
        "import torch\n",
        "import argparse\n",
        "from models.lstm_encoder import LSTMEncoder\n",
        "\n",
        "# Attempt to import the TDS encoder if available.\n",
        "try:\n",
        "    from models.tds_encoder import TDSEncoder\n",
        "except ImportError:\n",
        "    print(\"WARNING: Could not import TDSEncoder. Only LSTM encoder will be tested.\")\n",
        "    TDSEncoder = None\n",
        "\n",
        "def test_encoders(feature_dim=768, tds_params=None):\n",
        "    batch_size, time_steps = 8, 100\n",
        "    # Dummy input for LSTM encoder: (B, T, F)\n",
        "    x_lstm = torch.randn(batch_size, time_steps, feature_dim)\n",
        "    # Dummy input for TDS encoder: (B, F, T)\n",
        "    x_tds = x_lstm.transpose(1, 2)\n",
        "\n",
        "    # Test LSTM encoder\n",
        "    lstm = LSTMEncoder(input_dim=feature_dim, hidden_size=256, num_layers=2, bidirectional=True)\n",
        "    out_lstm = lstm(x_lstm)\n",
        "    print(f\"LSTM encoder output shape: {out_lstm.shape}\")\n",
        "\n",
        "    if TDSEncoder is not None and tds_params is not None:\n",
        "        tds = TDSEncoder(**tds_params)\n",
        "        out_tds = tds(x_tds)\n",
        "        print(f\"TDS encoder output shape: {out_tds.shape}\")\n",
        "        output_dims_match = out_tds.shape[-1] == out_lstm.shape[-1]\n",
        "        time_dims_match = out_tds.shape[2] == out_lstm.shape[1]\n",
        "        compatible = output_dims_match and time_dims_match\n",
        "        print(f\"Output dims match: {output_dims_match}\")\n",
        "        print(f\"Time dims match: {time_dims_match}\")\n",
        "        print(f\"Encoders are compatible: {compatible}\")\n",
        "        return compatible\n",
        "    return True\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Test encoder compatibility\")\n",
        "    parser.add_argument(\"--feature_dim\", type=int, default=768, help=\"Input feature dimension\")\n",
        "    args = parser.parse_args()\n",
        "    # Update tds_params if your TDS encoder parameters are available\n",
        "    tds_params = {}\n",
        "    test_encoders(feature_dim=args.feature_dim, tds_params=tds_params)\n"
      ],
      "metadata": {
        "id": "l01rVHp7WF_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/emg2qwerty-main/models/emg_model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Import encoders\n",
        "from .tds_encoder import TDSEncoder\n",
        "from .lstm_encoder import LSTMEncoder\n",
        "\n",
        "class EMGLightningModel(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder_type=\"TDS\",          # \"TDS\" or \"LSTM\"\n",
        "        feature_dim=768,             # Input feature dimension\n",
        "        num_classes=30,              # Number of output classes (including blank)\n",
        "        learning_rate=1e-3,\n",
        "        tds_params=None,             # TDS encoder parameters (if using TDS)\n",
        "        lstm_hidden_size=256,\n",
        "        lstm_num_layers=2,\n",
        "        lstm_dropout=0.2,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.encoder_type = encoder_type.upper()\n",
        "\n",
        "        if self.encoder_type == \"TDS\":\n",
        "            if tds_params is None:\n",
        "                tds_params = {}\n",
        "            self.encoder = TDSEncoder(**tds_params)\n",
        "            encoder_out_dim = self.encoder.output_dim\n",
        "        elif self.encoder_type == \"LSTM\":\n",
        "            self.encoder = LSTMEncoder(\n",
        "                input_dim=feature_dim,\n",
        "                hidden_size=lstm_hidden_size,\n",
        "                num_layers=lstm_num_layers,\n",
        "                bidirectional=True,\n",
        "                dropout=lstm_dropout\n",
        "            )\n",
        "            encoder_out_dim = self.encoder.output_dim\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown encoder type: {encoder_type}\")\n",
        "\n",
        "        self.classifier = nn.Linear(encoder_out_dim, num_classes)\n",
        "        self.ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def forward(self, x, lengths=None):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        For LSTM, expects input shape: (B, T, F)\n",
        "        For TDS, expects input shape: (B, F, T)\n",
        "        \"\"\"\n",
        "        if self.encoder_type == \"LSTM\":\n",
        "            if x.dim() == 3 and x.shape[1] < x.shape[2]:\n",
        "                # Assume input is in (B, F, T); transpose to (B, T, F)\n",
        "                x = x.transpose(1, 2)\n",
        "        elif self.encoder_type == \"TDS\":\n",
        "            if x.dim() == 3 and x.shape[1] > x.shape[2]:\n",
        "                # Assume input is in (B, T, F); transpose to (B, F, T)\n",
        "                x = x.transpose(1, 2)\n",
        "\n",
        "        features = self.encoder(x, lengths)\n",
        "        logits = self.classifier(features)\n",
        "        return logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, lengths, targets, target_lengths = batch\n",
        "        logits = self(x, lengths)\n",
        "        log_probs = F.log_softmax(logits, dim=-1).transpose(0, 1)\n",
        "        loss = self.ctc_loss(log_probs, targets, lengths, target_lengths)\n",
        "        self.log(\"train_loss\", loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, lengths, targets, target_lengths = batch\n",
        "        logits = self(x, lengths)\n",
        "        log_probs = F.log_softmax(logits, dim=-1).transpose(0, 1)\n",
        "        loss = self.ctc_loss(log_probs, targets, lengths, target_lengths)\n",
        "        self.log(\"val_loss\", loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, lengths, targets, target_lengths = batch\n",
        "        logits = self(x, lengths)\n",
        "        log_probs = F.log_softmax(logits, dim=-1).transpose(0, 1)\n",
        "        # Optionally, decode and compute error metrics here\n",
        "        return {\"test_log_probs\": log_probs}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=1e-5)\n",
        "        scheduler = {\n",
        "            \"scheduler\": torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.trainer.max_epochs),\n",
        "            \"interval\": \"epoch\",\n",
        "            \"frequency\": 1,\n",
        "        }\n",
        "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n"
      ],
      "metadata": {
        "id": "B0BibeSHWJE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/emg2qwerty-main/train.py\n",
        "import os\n",
        "import argparse\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "from models.emg_model import EMGLightningModel\n",
        "from data.datamodule import EMGDataModule\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Train EMG2QWERTY model\")\n",
        "    parser.add_argument('--encoder', type=str, default='TDS', choices=['TDS', 'LSTM'],\n",
        "                        help='Encoder type: TDS (original) or LSTM (new)')\n",
        "    parser.add_argument('--feature_dim', type=int, default=768, help='Input feature dimension')\n",
        "    parser.add_argument('--lstm_hidden_size', type=int, default=256, help='LSTM hidden size')\n",
        "    parser.add_argument('--lstm_num_layers', type=int, default=2, help='Number of LSTM layers')\n",
        "    parser.add_argument('--lstm_dropout', type=float, default=0.2, help='Dropout rate for LSTM')\n",
        "    parser.add_argument('--batch_size', type=int, default=32, help='Batch size for training')\n",
        "    parser.add_argument('--epochs', type=int, default=50, help='Number of training epochs')\n",
        "    parser.add_argument('--learning_rate', type=float, default=1e-3, help='Initial learning rate')\n",
        "    parser.add_argument('--gradient_clip_val', type=float, default=5.0, help='Gradient clipping value')\n",
        "    parser.add_argument('--data_dir', type=str, default='data', help='Dataset directory')\n",
        "    parser.add_argument('--name', type=str, default='model', help='Name for saving model and logs')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    data_module = EMGDataModule(data_dir=args.data_dir, batch_size=args.batch_size)\n",
        "    model = EMGLightningModel(\n",
        "        encoder_type=args.encoder,\n",
        "        feature_dim=args.feature_dim,\n",
        "        lstm_hidden_size=args.lstm_hidden_size,\n",
        "        lstm_num_layers=args.lstm_num_layers,\n",
        "        lstm_dropout=args.lstm_dropout,\n",
        "        learning_rate=args.learning_rate\n",
        "    )\n",
        "\n",
        "    logger = TensorBoardLogger(save_dir=\"lightning_logs\", name=f\"{args.encoder.lower()}_{args.name}\")\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        monitor=\"val_loss\",\n",
        "        filename=\"best_model-{epoch:02d}-{val_loss:.2f}\",\n",
        "        save_top_k=3,\n",
        "        mode=\"min\"\n",
        "    )\n",
        "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=10, mode=\"min\")\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=args.epochs,\n",
        "        gradient_clip_val=args.gradient_clip_val,\n",
        "        logger=logger,\n",
        "        callbacks=[checkpoint_callback, early_stop_callback],\n",
        "        accelerator=\"auto\",\n",
        "        devices=\"auto\"\n",
        "    )\n",
        "\n",
        "    trainer.fit(model, data_module)\n",
        "    trainer.test(model, data_module)\n",
        "\n",
        "    print(f\"Best model checkpoint: {checkpoint_callback.best_model_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "VzhqExUWWNOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/emg2qwerty-main/compare_models.py\n",
        "import os\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "import argparse\n",
        "import time\n",
        "import json\n",
        "from models.emg_model import EMGLightningModel\n",
        "from data.datamodule import EMGDataModule\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Compare TDS and LSTM models\")\n",
        "    parser.add_argument('--tds_checkpoint', type=str, required=True, help='Path to TDS model checkpoint')\n",
        "    parser.add_argument('--lstm_checkpoint', type=str, required=True, help='Path to LSTM model checkpoint')\n",
        "    parser.add_argument('--data_dir', type=str, default='data', help='Test dataset directory')\n",
        "    parser.add_argument('--batch_size', type=int, default=32, help='Batch size for evaluation')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    for ckpt in [args.tds_checkpoint, args.lstm_checkpoint]:\n",
        "        if not os.path.exists(ckpt):\n",
        "            raise FileNotFoundError(f\"Checkpoint not found: {ckpt}\")\n",
        "\n",
        "    data_module = EMGDataModule(data_dir=args.data_dir, batch_size=args.batch_size)\n",
        "    data_module.setup(stage='test')\n",
        "    test_dataloader = data_module.test_dataloader()\n",
        "\n",
        "    results = {\"TDS\": {}, \"LSTM\": {}, \"comparison\": {}}\n",
        "\n",
        "    print(\"\\nEvaluating TDS model...\")\n",
        "    tds_model = EMGLightningModel.load_from_checkpoint(args.tds_checkpoint)\n",
        "    tds_model.eval()\n",
        "    tds_params = sum(p.numel() for p in tds_model.parameters())\n",
        "    results[\"TDS\"][\"parameters\"] = tds_params\n",
        "    print(f\"TDS model parameters: {tds_params:,}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    tds_trainer = pl.Trainer(accelerator=\"auto\", devices=1)\n",
        "    tds_results = tds_trainer.test(tds_model, test_dataloader)[0]\n",
        "    tds_time = time.time() - start_time\n",
        "    results[\"TDS\"][\"CER\"] = tds_results.get(\"test_cer\", None)\n",
        "    results[\"TDS\"][\"WER\"] = tds_results.get(\"test_wer\", None)\n",
        "    results[\"TDS\"][\"inference_time\"] = tds_time\n",
        "    print(f\"TDS - CER: {results['TDS']['CER']:.2f}%, WER: {results['TDS']['WER']:.2f}%, Time: {tds_time:.2f}s\")\n",
        "\n",
        "    print(\"\\nEvaluating LSTM model...\")\n",
        "    lstm_model = EMGLightningModel.load_from_checkpoint(args.lstm_checkpoint)\n",
        "    lstm_model.eval()\n",
        "    lstm_params = sum(p.numel() for p in lstm_model.parameters())\n",
        "    results[\"LSTM\"][\"parameters\"] = lstm_params\n",
        "    print(f\"LSTM model parameters: {lstm_params:,}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    lstm_trainer = pl.Trainer(accelerator=\"auto\", devices=1)\n",
        "    lstm_results = lstm_trainer.test(lstm_model, test_dataloader)[0]\n",
        "    lstm_time = time.time() - start_time\n",
        "    results[\"LSTM\"][\"CER\"] = lstm_results.get(\"test_cer\", None)\n",
        "    results[\"LSTM\"][\"WER\"] = lstm_results.get(\"test_wer\", None)\n",
        "    results[\"LSTM\"][\"inference_time\"] = lstm_time\n",
        "    print(f\"LSTM - CER: {results['LSTM']['CER']:.2f}%, WER: {results['LSTM']['WER']:.2f}%, Time: {lstm_time:.2f}s\")\n",
        "\n",
        "    if results[\"TDS\"].get(\"CER\") is not None and results[\"LSTM\"].get(\"CER\") is not None:\n",
        "        results[\"comparison\"][\"CER_diff\"] = results[\"LSTM\"][\"CER\"] - results[\"TDS\"][\"CER\"]\n",
        "        results[\"comparison\"][\"CER_relative\"] = (results[\"LSTM\"][\"CER\"] / results[\"TDS\"][\"CER\"] - 1) * 100\n",
        "    if results[\"TDS\"].get(\"WER\") is not None and results[\"LSTM\"].get(\"WER\") is not None:\n",
        "        results[\"comparison\"][\"WER_diff\"] = results[\"LSTM\"][\"WER\"] - results[\"TDS\"][\"WER\"]\n",
        "        results[\"comparison\"][\"WER_relative\"] = (results[\"LSTM\"][\"WER\"] / results[\"TDS\"][\"WER\"] - 1) * 100\n",
        "    if results[\"TDS\"].get(\"inference_time\") and results[\"LSTM\"].get(\"inference_time\"):\n",
        "        results[\"comparison\"][\"speed_ratio\"] = results[\"TDS\"][\"inference_time\"] / results[\"LSTM\"][\"inference_time\"]\n",
        "    if results[\"TDS\"].get(\"parameters\") and results[\"LSTM\"].get(\"parameters\"):\n",
        "        results[\"comparison\"][\"param_diff\"] = results[\"LSTM\"][\"parameters\"] - results[\"TDS\"][\"parameters\"]\n",
        "        results[\"comparison\"][\"param_ratio\"] = results[\"LSTM\"][\"parameters\"] / results[\"TDS\"][\"parameters\"]\n",
        "\n",
        "    print(\"\\n=== Model Comparison Results ===\")\n",
        "    print(f\"Character Error Rate: TDS = {results['TDS'].get('CER', 'N/A'):.2f}%, LSTM = {results['LSTM'].get('CER', 'N/A'):.2f}%\")\n",
        "    print(f\"CER Difference: {results['comparison'].get('CER_diff', 'N/A'):.2f}% ({results['comparison'].get('CER_relative', 'N/A'):.1f}% relative)\")\n",
        "    print(f\"Word Error Rate: TDS = {results['TDS'].get('WER', 'N/A'):.2f}%, LSTM = {results['LSTM'].get('WER', 'N/A'):.2f}%\")\n",
        "    print(f\"WER Difference: {results['comparison'].get('WER_diff', 'N/A'):.2f}% ({results['comparison'].get('WER_relative', 'N/A'):.1f}% relative)\")\n",
        "    print(f\"Model Size: TDS = {results['TDS'].get('parameters', 'N/A'):,}, LSTM = {results['LSTM'].get('parameters', 'N/A'):,}\")\n",
        "    print(f\"Parameter Ratio: LSTM is {results['comparison'].get('param_ratio', 'N/A'):.2f}x the size of TDS\")\n",
        "    print(f\"Speed Ratio (TDS inference / LSTM inference): {results['comparison'].get('speed_ratio', 'N/A'):.2f}\")\n",
        "\n",
        "    with open(\"model_comparison_results.json\", \"w\") as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "    print(\"\\nComparison results saved to model_comparison_results.json\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "b5_ZTq4CWQoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_encoders.py\n"
      ],
      "metadata": {
        "id": "w6mon3wMWT2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/colab/ece247/proj/tmp/data/datamodule.py\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "\n",
        "class DummyEMGDataset(Dataset):\n",
        "    def __init__(self, mode=\"train\"):\n",
        "        self.mode = mode\n",
        "        # For demonstration, we create 100 dummy samples.\n",
        "        self.samples = list(range(100))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Create a dummy tensor:\n",
        "        # For example, for LSTM, assume input is (time_steps, features) with time_steps=100, feature_dim=768.\n",
        "        x = torch.randn(100, 768)\n",
        "        # Dummy target sequence (random integers in range 1 to num_classes-1, here assuming 30 classes with 0 as blank)\n",
        "        y = torch.randint(1, 30, (10,))\n",
        "        # For CTC, we also need lengths; here we assume full-length sequence.\n",
        "        input_length = 100\n",
        "        target_length = 10\n",
        "        return x, input_length, y, target_length\n",
        "\n",
        "class EMGDataModule:\n",
        "    def __init__(self, data_dir, batch_size=32):\n",
        "        self.data_dir = data_dir  # This can be used to load real data later.\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        # Here you can implement different setups for train, val, and test.\n",
        "        # For now, we use the same dummy dataset.\n",
        "        self.train_dataset = DummyEMGDataset(mode=\"train\")\n",
        "        self.val_dataset = DummyEMGDataset(mode=\"val\")\n",
        "        self.test_dataset = DummyEMGDataset(mode=\"test\")\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "ARBE8VaHj4Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/colab/ece247/proj/tmp/data/datamodule.py\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class DummyEMGDataset(Dataset):\n",
        "    def __init__(self, mode=\"train\"):\n",
        "        self.mode = mode\n",
        "        # For demonstration, create 100 dummy samples.\n",
        "        self.samples = list(range(100))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Create a dummy tensor for input:\n",
        "        # Assume input shape for LSTM is (time_steps, feature_dim) with time_steps=100 and feature_dim=768.\n",
        "        x = torch.randn(100, 768)\n",
        "        # Create a dummy target sequence (length 10)\n",
        "        y = torch.randint(1, 30, (10,))\n",
        "        # Define input and target lengths\n",
        "        input_length = 100\n",
        "        target_length = 10\n",
        "        return x, input_length, y, target_length\n",
        "\n",
        "class EMGDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, data_dir, batch_size=32):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir  # Use this later to load real data\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # Implement data download or preprocessing here if needed.\n",
        "        pass\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        # Setup datasets for train, val, and test.\n",
        "        self.train_dataset = DummyEMGDataset(mode=\"train\")\n",
        "        self.val_dataset = DummyEMGDataset(mode=\"val\")\n",
        "        self.test_dataset = DummyEMGDataset(mode=\"test\")\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "    def on_exception(self, exception):\n",
        "        # Optional: define a dummy on_exception hook\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "iJ5hp7F-l1Er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/emg2qwerty-main/models/lstm_encoder.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "\n",
        "class LSTMEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Bidirectional LSTM encoder for EMG signals.\n",
        "\n",
        "    Processes input sequences of shape (batch, time, features) and returns\n",
        "    output sequences of shape (batch, time, output_dim), where output_dim is\n",
        "    hidden_size * 2 (if bidirectional) to match the expected classifier input.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_size=256, num_layers=2, bidirectional=True, dropout=0.2):\n",
        "        super(LSTMEncoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=bidirectional,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "        # Calculate output dimension: hidden_size * 2 if bidirectional; else hidden_size.\n",
        "        self.output_dim = hidden_size * 2 if bidirectional else hidden_size\n",
        "\n",
        "    def forward(self, x, lengths=None):\n",
        "        \"\"\"\n",
        "        Forward pass through the LSTM encoder.\n",
        "\n",
        "        Args:\n",
        "            x: Tensor of shape (batch_size, time_steps, input_dim)\n",
        "            lengths: Optional tensor with actual sequence lengths.\n",
        "\n",
        "        Returns:\n",
        "            lstm_out: Tensor of shape (batch_size, time_steps, output_dim)\n",
        "        \"\"\"\n",
        "        if lengths is not None:\n",
        "            # Ensure lengths is a CPU int64 tensor\n",
        "            lengths = lengths.cpu().to(torch.int64)\n",
        "            # Pack the sequence for variable-length processing.\n",
        "            x_packed = rnn_utils.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
        "            lstm_out, _ = self.lstm(x_packed)\n",
        "            lstm_out, _ = rnn_utils.pad_packed_sequence(lstm_out, batch_first=True)\n",
        "        else:\n",
        "            lstm_out, _ = self.lstm(x)\n",
        "        return lstm_out\n"
      ],
      "metadata": {
        "id": "BKnAE9vGnJtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/emg2qwerty-main/models/emg_model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Import encoders (if TDSEncoder is not used, you can leave it or create a dummy file)\n",
        "from .tds_encoder import TDSEncoder\n",
        "from .lstm_encoder import LSTMEncoder\n",
        "\n",
        "class EMGLightningModel(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder_type=\"TDS\",          # \"TDS\" or \"LSTM\"\n",
        "        feature_dim=768,             # Input feature dimension\n",
        "        num_classes=30,              # Number of output classes (including blank)\n",
        "        learning_rate=1e-3,\n",
        "        tds_params=None,             # TDS encoder parameters (if using TDS)\n",
        "        lstm_hidden_size=256,\n",
        "        lstm_num_layers=2,\n",
        "        lstm_dropout=0.2,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.encoder_type = encoder_type.upper()\n",
        "\n",
        "        if self.encoder_type == \"TDS\":\n",
        "            if tds_params is None:\n",
        "                tds_params = {}\n",
        "            self.encoder = TDSEncoder(**tds_params)\n",
        "            encoder_out_dim = self.encoder.output_dim\n",
        "        elif self.encoder_type == \"LSTM\":\n",
        "            self.encoder = LSTMEncoder(\n",
        "                input_dim=feature_dim,\n",
        "                hidden_size=lstm_hidden_size,\n",
        "                num_layers=lstm_num_layers,\n",
        "                bidirectional=True,\n",
        "                dropout=lstm_dropout\n",
        "            )\n",
        "            encoder_out_dim = self.encoder.output_dim\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown encoder type: {encoder_type}\")\n",
        "\n",
        "        self.classifier = nn.Linear(encoder_out_dim, num_classes)\n",
        "        self.ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def forward(self, x, lengths=None):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        For LSTM: Assumes input x is already in shape (B, T, F).\n",
        "        For TDS: Assumes input x is in shape (B, T, F) and transposes to (B, F, T).\n",
        "        \"\"\"\n",
        "        if self.encoder_type == \"TDS\":\n",
        "            if x.dim() == 3 and x.shape[1] > x.shape[2]:\n",
        "                # Convert from (B, T, F) to (B, F, T)\n",
        "                x = x.transpose(1, 2)\n",
        "        # For LSTM, no transposition is performed.\n",
        "        features = self.encoder(x, lengths)\n",
        "        logits = self.classifier(features)\n",
        "        return logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, lengths, targets, target_lengths = batch\n",
        "        logits = self(x, lengths)\n",
        "        log_probs = F.log_softmax(logits, dim=-1).transpose(0, 1)\n",
        "        loss = self.ctc_loss(log_probs, targets, lengths, target_lengths)\n",
        "        self.log(\"train_loss\", loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, lengths, targets, target_lengths = batch\n",
        "        logits = self(x, lengths)\n",
        "        log_probs = F.log_softmax(logits, dim=-1).transpose(0, 1)\n",
        "        loss = self.ctc_loss(log_probs, targets, lengths, target_lengths)\n",
        "        self.log(\"val_loss\", loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, lengths, targets, target_lengths = batch\n",
        "        logits = self(x, lengths)\n",
        "        log_probs = F.log_softmax(logits, dim=-1).transpose(0, 1)\n",
        "        return {\"test_log_probs\": log_probs}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=1e-5)\n",
        "        scheduler = {\n",
        "            \"scheduler\": torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.trainer.max_epochs),\n",
        "            \"interval\": \"epoch\",\n",
        "            \"frequency\": 1,\n",
        "        }\n",
        "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n"
      ],
      "metadata": {
        "id": "XeHDBx8vohE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"/content/drive/MyDrive/colab/ece247/proj/tmp/\")\n"
      ],
      "metadata": {
        "id": "MQ9h-fUFkDFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --encoder LSTM --batch_size 32 --epochs 50 --name lstm --learning_rate 1e-3 --gradient_clip_val 5.0 --data_dir /content/drive/MyDrive/colab/ece247/proj/tmp/data/\n"
      ],
      "metadata": {
        "id": "F5nY3_fMlewE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/emg2qwerty-main/train.py\n",
        "import sys\n",
        "# Add the parent directory of the 'data' package to the Python path.\n",
        "sys.path.insert(0, \"/content/drive/MyDrive/colab/ece247/proj/tmp/\")\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "from models.emg_model import EMGLightningModel\n",
        "from data.datamodule import EMGDataModule\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Train or Test EMG2QWERTY model\")\n",
        "    # Model parameters\n",
        "    parser.add_argument('--encoder', type=str, default='TDS', choices=['TDS', 'LSTM'],\n",
        "                        help='Encoder type: TDS (original) or LSTM (new)')\n",
        "    parser.add_argument('--feature_dim', type=int, default=768, help='Input feature dimension')\n",
        "    parser.add_argument('--lstm_hidden_size', type=int, default=256, help='LSTM hidden size')\n",
        "    parser.add_argument('--lstm_num_layers', type=int, default=2, help='Number of LSTM layers')\n",
        "    parser.add_argument('--lstm_dropout', type=float, default=0.2, help='Dropout rate for LSTM')\n",
        "    # Training parameters\n",
        "    parser.add_argument('--batch_size', type=int, default=32, help='Batch size for training')\n",
        "    parser.add_argument('--epochs', type=int, default=50, help='Number of training epochs')\n",
        "    parser.add_argument('--learning_rate', type=float, default=1e-3, help='Initial learning rate')\n",
        "    parser.add_argument('--gradient_clip_val', type=float, default=5.0, help='Gradient clipping value')\n",
        "    # Data and output paths\n",
        "    parser.add_argument('--data_dir', type=str, default='data', help='Dataset directory')\n",
        "    parser.add_argument('--name', type=str, default='model', help='Name for saving model and logs')\n",
        "    # Test-only mode\n",
        "    parser.add_argument('--test_only', action='store_true', help='Run in test-only mode (requires checkpoint)')\n",
        "    parser.add_argument('--ckpt_path', type=str, default=None, help='Checkpoint path for test-only mode')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    data_module = EMGDataModule(data_dir=args.data_dir, batch_size=args.batch_size)\n",
        "    data_module.setup(stage=\"fit\")\n",
        "\n",
        "    if args.test_only:\n",
        "        if args.ckpt_path is None:\n",
        "            raise ValueError(\"For test-only mode, please specify a checkpoint path using --ckpt_path\")\n",
        "        # Load model from checkpoint and run test\n",
        "        model = EMGLightningModel.load_from_checkpoint(args.ckpt_path)\n",
        "        trainer = pl.Trainer(accelerator=\"auto\", devices=\"auto\")\n",
        "        trainer.test(model, datamodule=data_module)\n",
        "    else:\n",
        "        model = EMGLightningModel(\n",
        "            encoder_type=args.encoder,\n",
        "            feature_dim=args.feature_dim,\n",
        "            lstm_hidden_size=args.lstm_hidden_size,\n",
        "            lstm_num_layers=args.lstm_num_layers,\n",
        "            lstm_dropout=args.lstm_dropout,\n",
        "            learning_rate=args.learning_rate\n",
        "        )\n",
        "\n",
        "        logger = TensorBoardLogger(save_dir=\"lightning_logs\", name=f\"{args.encoder.lower()}_{args.name}\")\n",
        "        checkpoint_callback = ModelCheckpoint(\n",
        "            monitor=\"val_loss\",\n",
        "            filename=\"best_model-{epoch:02d}-{val_loss:.2f}\",\n",
        "            save_top_k=3,\n",
        "            mode=\"min\"\n",
        "        )\n",
        "        early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=10, mode=\"min\")\n",
        "\n",
        "        trainer = pl.Trainer(\n",
        "            max_epochs=args.epochs,\n",
        "            gradient_clip_val=args.gradient_clip_val,\n",
        "            logger=logger,\n",
        "            callbacks=[checkpoint_callback, early_stop_callback],\n",
        "            accelerator=\"auto\",\n",
        "            devices=\"auto\"\n",
        "        )\n",
        "\n",
        "        trainer.fit(model, datamodule=data_module)\n",
        "        trainer.test(model, datamodule=data_module)\n",
        "        print(f\"Best model checkpoint: {checkpoint_callback.best_model_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "GNuIpE20qS2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --encoder LSTM --batch_size 32 --epochs 50 --name lstm --learning_rate 1e-3 --gradient_clip_val 5.0 --data_dir /content/drive/MyDrive/colab/ece247/proj/tmp/data/\n"
      ],
      "metadata": {
        "id": "Vz3txnyXpiWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --test_only --ckpt_path lightning_logs/lstm_lstm/version_4/checkpoints/best_model-epoch=21-val_loss=3.57.ckpt --data_dir /content/drive/MyDrive/colab/ece247/proj/tmp/data/\n"
      ],
      "metadata": {
        "id": "vGUqr7_nrMk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/emg2qwerty-main/compare_models.py\n",
        "import sys\n",
        "# Add the parent directory (where the data folder resides) to the Python path.\n",
        "sys.path.insert(0, \"/content/drive/MyDrive/colab/ece247/proj/tmp/\")\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "import argparse\n",
        "import time\n",
        "import json\n",
        "from models.emg_model import EMGLightningModel\n",
        "from data.datamodule import EMGDataModule\n",
        "\n",
        "def format_metric(value):\n",
        "    return f\"{value:.2f}\" if value is not None else \"N/A\"\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Compare TDS and LSTM models\")\n",
        "    parser.add_argument('--tds_checkpoint', type=str, default=None,\n",
        "                        help='Path to TDS model checkpoint (optional)')\n",
        "    parser.add_argument('--lstm_checkpoint', type=str, required=True,\n",
        "                        help='Path to LSTM model checkpoint')\n",
        "    parser.add_argument('--data_dir', type=str, default='data', help='Test dataset directory')\n",
        "    parser.add_argument('--batch_size', type=int, default=32, help='Batch size for evaluation')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Check that the LSTM checkpoint exists\n",
        "    if not os.path.exists(args.lstm_checkpoint):\n",
        "        raise FileNotFoundError(f\"Checkpoint not found: {args.lstm_checkpoint}\")\n",
        "\n",
        "    # If provided, check TDS checkpoint exists\n",
        "    if args.tds_checkpoint is not None and not os.path.exists(args.tds_checkpoint):\n",
        "        raise FileNotFoundError(f\"Checkpoint not found: {args.tds_checkpoint}\")\n",
        "\n",
        "    data_module = EMGDataModule(data_dir=args.data_dir, batch_size=args.batch_size)\n",
        "    data_module.setup(stage='test')\n",
        "    test_dataloader = data_module.test_dataloader()\n",
        "\n",
        "    results = {\"TDS\": {}, \"LSTM\": {}, \"comparison\": {}}\n",
        "\n",
        "    # Evaluate TDS model only if a checkpoint is provided\n",
        "    if args.tds_checkpoint is not None:\n",
        "        print(\"\\nEvaluating TDS model...\")\n",
        "        tds_model = EMGLightningModel.load_from_checkpoint(args.tds_checkpoint)\n",
        "        tds_model.eval()\n",
        "        tds_params = sum(p.numel() for p in tds_model.parameters())\n",
        "        results[\"TDS\"][\"parameters\"] = tds_params\n",
        "        print(f\"TDS model parameters: {tds_params:,}\")\n",
        "\n",
        "        start_time = time.time()\n",
        "        tds_trainer = pl.Trainer(accelerator=\"auto\", devices=1)\n",
        "        tds_results = tds_trainer.test(tds_model, test_dataloader)[0]\n",
        "        tds_time = time.time() - start_time\n",
        "        results[\"TDS\"][\"CER\"] = tds_results.get(\"test_cer\", None)\n",
        "        results[\"TDS\"][\"WER\"] = tds_results.get(\"test_wer\", None)\n",
        "        results[\"TDS\"][\"inference_time\"] = tds_time\n",
        "        print(f\"TDS - CER: {format_metric(results['TDS']['CER'])}%, WER: {format_metric(results['TDS']['WER'])}%, Time: {format_metric(tds_time)}s\")\n",
        "    else:\n",
        "        print(\"No TDS checkpoint provided. Skipping TDS evaluation.\")\n",
        "\n",
        "    print(\"\\nEvaluating LSTM model...\")\n",
        "    lstm_model = EMGLightningModel.load_from_checkpoint(args.lstm_checkpoint)\n",
        "    lstm_model.eval()\n",
        "    lstm_params = sum(p.numel() for p in lstm_model.parameters())\n",
        "    results[\"LSTM\"][\"parameters\"] = lstm_params\n",
        "    print(f\"LSTM model parameters: {lstm_params:,}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    lstm_trainer = pl.Trainer(accelerator=\"auto\", devices=1)\n",
        "    lstm_results = lstm_trainer.test(lstm_model, test_dataloader)[0]\n",
        "    lstm_time = time.time() - start_time\n",
        "    results[\"LSTM\"][\"CER\"] = lstm_results.get(\"test_cer\", None)\n",
        "    results[\"LSTM\"][\"WER\"] = lstm_results.get(\"test_wer\", None)\n",
        "    results[\"LSTM\"][\"inference_time\"] = lstm_time\n",
        "    print(f\"LSTM - CER: {format_metric(results['LSTM']['CER'])}%, WER: {format_metric(results['LSTM']['WER'])}%, Time: {format_metric(lstm_time)}s\")\n",
        "\n",
        "    # Compute comparisons only if both checkpoints are available.\n",
        "    if args.tds_checkpoint is not None:\n",
        "        if results[\"TDS\"].get(\"CER\") is not None and results[\"LSTM\"].get(\"CER\") is not None:\n",
        "            results[\"comparison\"][\"CER_diff\"] = results[\"LSTM\"][\"CER\"] - results[\"TDS\"][\"CER\"]\n",
        "            results[\"comparison\"][\"CER_relative\"] = (results[\"LSTM\"][\"CER\"] / results[\"TDS\"][\"CER\"] - 1) * 100\n",
        "        if results[\"TDS\"].get(\"WER\") is not None and results[\"LSTM\"].get(\"WER\") is not None:\n",
        "            results[\"comparison\"][\"WER_diff\"] = results[\"LSTM\"][\"WER\"] - results[\"TDS\"][\"WER\"]\n",
        "            results[\"comparison\"][\"WER_relative\"] = (results[\"LSTM\"][\"WER\"] / results[\"TDS\"][\"WER\"] - 1) * 100\n",
        "        if results[\"TDS\"].get(\"inference_time\") and results[\"LSTM\"].get(\"inference_time\"):\n",
        "            results[\"comparison\"][\"speed_ratio\"] = results[\"TDS\"][\"inference_time\"] / results[\"LSTM\"][\"inference_time\"]\n",
        "        if results[\"TDS\"].get(\"parameters\") and results[\"LSTM\"].get(\"parameters\"):\n",
        "            results[\"comparison\"][\"param_diff\"] = results[\"LSTM\"][\"parameters\"] - results[\"TDS\"][\"parameters\"]\n",
        "            results[\"comparison\"][\"param_ratio\"] = results[\"LSTM\"][\"parameters\"] / results[\"TDS\"][\"parameters\"]\n",
        "\n",
        "    print(\"\\n=== Model Comparison Results ===\")\n",
        "    if args.tds_checkpoint is not None:\n",
        "        print(f\"Character Error Rate: TDS = {format_metric(results['TDS'].get('CER'))}%, LSTM = {format_metric(results['LSTM'].get('CER'))}%\")\n",
        "        print(f\"CER Difference: {format_metric(results['comparison'].get('CER_diff'))}% ({format_metric(results['comparison'].get('CER_relative'))}% relative)\")\n",
        "        print(f\"Word Error Rate: TDS = {format_metric(results['TDS'].get('WER'))}%, LSTM = {format_metric(results['LSTM'].get('WER'))}%\")\n",
        "        print(f\"WER Difference: {format_metric(results['comparison'].get('WER_diff'))}% ({format_metric(results['comparison'].get('WER_relative'))}% relative)\")\n",
        "        print(f\"Model Size: TDS = {results['TDS'].get('parameters', 'N/A'):,}, LSTM = {results['LSTM'].get('parameters', 'N/A'):,}\")\n",
        "        print(f\"Parameter Ratio: LSTM is {format_metric(results['comparison'].get('param_ratio'))}x the size of TDS\")\n",
        "        print(f\"Speed Ratio (TDS inference / LSTM inference): {format_metric(results['comparison'].get('speed_ratio'))}\")\n",
        "    else:\n",
        "        print(\"Only LSTM checkpoint evaluated; no TDS comparison available.\")\n",
        "\n",
        "    with open(\"model_comparison_results.json\", \"w\") as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "    print(\"\\nComparison results saved to model_comparison_results.json\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "midH7urDsAsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python compare_models.py --lstm_checkpoint /content/emg2qwerty-main/lightning_logs/lstm_lstm/version_4/checkpoints/best_model-epoch=21-val_loss=3.57.ckpt --batch_size 32\n"
      ],
      "metadata": {
        "id": "Go2Ra7O6rcxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/emg2qwerty-main/models/emg_model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Since we're only using LSTM, we only import the LSTM encoder.\n",
        "from .lstm_encoder import LSTMEncoder\n",
        "\n",
        "class EMGLightningModel(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder_type=\"LSTM\",          # Only LSTM is supported in this update\n",
        "        feature_dim=768,             # Input feature dimension\n",
        "        num_classes=30,              # Number of output classes (including blank index 0)\n",
        "        learning_rate=1e-3,\n",
        "        lstm_hidden_size=256,\n",
        "        lstm_num_layers=2,\n",
        "        lstm_dropout=0.2,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.encoder_type = encoder_type.upper()\n",
        "\n",
        "        if self.encoder_type == \"LSTM\":\n",
        "            self.encoder = LSTMEncoder(\n",
        "                input_dim=feature_dim,\n",
        "                hidden_size=lstm_hidden_size,\n",
        "                num_layers=lstm_num_layers,\n",
        "                bidirectional=True,\n",
        "                dropout=lstm_dropout\n",
        "            )\n",
        "            encoder_out_dim = self.encoder.output_dim\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown encoder type: {encoder_type}\")\n",
        "\n",
        "        self.classifier = nn.Linear(encoder_out_dim, num_classes)\n",
        "        self.ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def forward(self, x, lengths=None):\n",
        "        # For LSTM, assume x is already in (B, T, F) format.\n",
        "        features = self.encoder(x, lengths)\n",
        "        logits = self.classifier(features)\n",
        "        return logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, lengths, targets, target_lengths = batch\n",
        "        logits = self(x, lengths)\n",
        "        log_probs = F.log_softmax(logits, dim=-1).transpose(0, 1)\n",
        "        loss = self.ctc_loss(log_probs, targets, lengths, target_lengths)\n",
        "        self.log(\"train_loss\", loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, lengths, targets, target_lengths = batch\n",
        "        logits = self(x, lengths)\n",
        "        log_probs = F.log_softmax(logits, dim=-1).transpose(0, 1)\n",
        "        loss = self.ctc_loss(log_probs, targets, lengths, target_lengths)\n",
        "        self.log(\"val_loss\", loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, lengths, targets, target_lengths = batch\n",
        "        logits = self(x, lengths)\n",
        "        log_probs = F.log_softmax(logits, dim=-1).transpose(0, 1)\n",
        "\n",
        "        # Decode predictions using greedy decoding\n",
        "        predictions = self._greedy_decode(log_probs)\n",
        "\n",
        "        # Compute CER (Character Error Rate) using Levenshtein distance.\n",
        "        total_edits = 0\n",
        "        total_length = 0\n",
        "        for pred, target, tgt_len in zip(predictions, targets, target_lengths):\n",
        "            target_list = target[:tgt_len].tolist()\n",
        "            edits = self._levenshtein_distance(pred, target_list)\n",
        "            total_edits += edits\n",
        "            total_length += len(target_list)\n",
        "        cer = total_edits / total_length if total_length > 0 else 0.0\n",
        "        # For now, we approximate WER (Word Error Rate) as 1.2 * CER (placeholder)\n",
        "        wer = cer * 1.2\n",
        "\n",
        "        self.log(\"test_cer\", cer, prog_bar=True)\n",
        "        self.log(\"test_wer\", wer, prog_bar=True)\n",
        "        return {\"test_cer\": cer, \"test_wer\": wer}\n",
        "\n",
        "    def _greedy_decode(self, log_probs):\n",
        "        \"\"\"\n",
        "        Performs greedy decoding on log probabilities.\n",
        "\n",
        "        Args:\n",
        "            log_probs: Tensor of shape (T, B, C)\n",
        "\n",
        "        Returns:\n",
        "            A list of lists of token indices for each sample in the batch.\n",
        "        \"\"\"\n",
        "        # Greedy decoding: take the argmax for each time step.\n",
        "        pred_indices = torch.argmax(log_probs, dim=-1)  # Shape: (T, B)\n",
        "        pred_indices = pred_indices.transpose(0, 1)       # Shape: (B, T)\n",
        "        decoded = []\n",
        "        for seq in pred_indices:\n",
        "            seq = seq.tolist()\n",
        "            # Collapse repeated tokens and remove blanks (assume blank index = 0)\n",
        "            prev = None\n",
        "            decoded_seq = []\n",
        "            for token in seq:\n",
        "                if token == 0:  # blank token\n",
        "                    prev = None\n",
        "                    continue\n",
        "                if token != prev:\n",
        "                    decoded_seq.append(token)\n",
        "                    prev = token\n",
        "            decoded.append(decoded_seq)\n",
        "        return decoded\n",
        "\n",
        "    def _levenshtein_distance(self, s1, s2):\n",
        "        \"\"\"\n",
        "        Compute the Levenshtein distance between sequences s1 and s2.\n",
        "        \"\"\"\n",
        "        if len(s1) < len(s2):\n",
        "            return self._levenshtein_distance(s2, s1)\n",
        "        if len(s2) == 0:\n",
        "            return len(s1)\n",
        "        previous_row = list(range(len(s2) + 1))\n",
        "        for i, c1 in enumerate(s1):\n",
        "            current_row = [i + 1]\n",
        "            for j, c2 in enumerate(s2):\n",
        "                insertions = previous_row[j + 1] + 1\n",
        "                deletions = current_row[j] + 1\n",
        "                substitutions = previous_row[j] + (c1 != c2)\n",
        "                current_row.append(min(insertions, deletions, substitutions))\n",
        "            previous_row = current_row\n",
        "        return previous_row[-1]\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=1e-5)\n",
        "        scheduler = {\n",
        "            \"scheduler\": torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.trainer.max_epochs),\n",
        "            \"interval\": \"epoch\",\n",
        "            \"frequency\": 1,\n",
        "        }\n",
        "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n"
      ],
      "metadata": {
        "id": "x5uVk44SuLWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --encoder LSTM --batch_size 32 --epochs 50 --name lstm --learning_rate 1e-3 --gradient_clip_val 5.0 --data_dir /content/drive/MyDrive/colab/ece247/proj/tmp/data/\n"
      ],
      "metadata": {
        "id": "vJY_CNvxuQHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs/\n"
      ],
      "metadata": {
        "id": "YO5fPdzhuoJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -dpRvf /content/emg2qwerty-main/   /content/drive/MyDrive/colab/ece247/proj/tmp/bak2/"
      ],
      "metadata": {
        "id": "UZbj0ouzyLqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sync"
      ],
      "metadata": {
        "id": "jJtsWb4Gyj45"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
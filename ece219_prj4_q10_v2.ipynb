{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNSW0AHis/mgaYHtXPMGy9b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/petervinhchau/public/blob/main/ece219_prj4_q10_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJApmVxQpIcD"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Super Bowl Fan Classification Project - Fixed Google Colab Version\n",
        "\n",
        "This notebook implements a data mining project to classify tweets as supporting either the Patriots or Seahawks\n",
        "during Super Bowl 49, and analyzes how fan language and engagement patterns changed throughout the game.\n",
        "\n",
        "## Setup and Data Loading\n",
        "\"\"\"\n",
        "\n",
        "# Install required packages\n",
        "!pip install pandas matplotlib pytz scikit-learn nltk imblearn seaborn tqdm\n",
        "\n",
        "import os\n",
        "\n",
        "# Check if dataset already exists\n",
        "if not os.path.exists('/content/tweets_dataset.zip'):\n",
        "    # Download the dataset (tweets data) and unzip it\n",
        "    !wget -O tweets_dataset.zip -L \"https://ucla.box.com/shared/static/24oxnhsoj6kpxhl6gyvuck25i3s4426d\"\n",
        "    !mkdir -p /content/tweet_data/\n",
        "    !unzip -o tweets_dataset.zip -d /content/tweet_data/\n",
        "else:\n",
        "    print(\"Dataset already exists, skipping download and unzip.\")\n",
        "\n",
        "\"\"\"\n",
        "## Import Libraries\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "import glob\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# For text processing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# For modeling\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# For dealing with imbalanced data\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('stopwords', quiet=True)\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "stop_words = list(stopwords.words('english'))\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "## Step 1: Find and Load the Dataset\n",
        "\"\"\"\n",
        "\n",
        "# First, let's find all files in the dataset\n",
        "print(\"=== Checking available files ===\")\n",
        "!find /content/tweet_data -type f | sort\n",
        "\n",
        "# Function to safely load tweets from JSON file\n",
        "def load_tweets(file_path, max_tweets=None):\n",
        "    \"\"\"\n",
        "    Load tweets from a JSON file where each line is a separate JSON object.\n",
        "\n",
        "    Parameters:\n",
        "    - file_path: Path to the JSON file\n",
        "    - max_tweets: Optional limit on number of tweets to load (for testing)\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame containing the parsed tweets\n",
        "    \"\"\"\n",
        "    print(f\"Loading tweets from {file_path}...\")\n",
        "    tweets = []\n",
        "    count = 0\n",
        "    errors = 0\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
        "        for line in tqdm(f):\n",
        "            if max_tweets and count >= max_tweets:\n",
        "                break\n",
        "\n",
        "            # Skip empty lines\n",
        "            if not line.strip():\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                tweet = json.loads(line.strip())\n",
        "                tweets.append(tweet)\n",
        "                count += 1\n",
        "            except json.JSONDecodeError:\n",
        "                errors += 1\n",
        "                if errors <= 5:  # Only show first few errors\n",
        "                    print(f\"Error parsing JSON on line {count+errors}\")\n",
        "\n",
        "    print(f\"Successfully loaded {len(tweets)} tweets with {errors} errors\")\n",
        "    return pd.DataFrame(tweets)\n",
        "\n",
        "# Find tweet files and load the first one found\n",
        "tweet_files = glob.glob('/content/tweet_data/**/tweets_*.txt', recursive=True)\n",
        "\n",
        "if not tweet_files:\n",
        "    print(\"No tweet files found! Please check the dataset extraction.\")\n",
        "    # Create dummy data for testing\n",
        "    df = pd.DataFrame({\n",
        "        'text': ['#gopatriots great game', '#gohawks we can win', 'watching #superbowl'],\n",
        "        'author': [{'followers': 100}, {'followers': 200}, {'followers': 150}],\n",
        "        'citation_date': [1422832800, 1422836400, 1422840000],  # Feb 1, 2015 timestamps\n",
        "        'metrics': [{'citations': {'total': 5}}, {'citations': {'total': 10}}, {'citations': {'total': 2}}]\n",
        "    })\n",
        "    print(\"Created dummy data for testing since no files were found.\")\n",
        "else:\n",
        "    print(f\"Found {len(tweet_files)} tweet files\")\n",
        "    # Load the first file\n",
        "    df = load_tweets(tweet_files[0], max_tweets=50000)  # Adjust max_tweets as needed\n",
        "\n",
        "# Basic dataset info\n",
        "print(\"\\n=== Dataset Information ===\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "\n",
        "# Check for basic required columns\n",
        "required_cols = ['text', 'author', 'citation_date', 'metrics']\n",
        "missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "if missing_cols:\n",
        "    print(f\"Warning: Missing required columns: {missing_cols}\")\n",
        "\n",
        "    # Try to repair missing columns with dummy values\n",
        "    for col in missing_cols:\n",
        "        if col == 'text':\n",
        "            df['text'] = 'No text available'\n",
        "        elif col == 'author':\n",
        "            df['author'] = [{'followers': 0}] * len(df)\n",
        "        elif col == 'citation_date':\n",
        "            # Feb 1, 2015 - Super Bowl day\n",
        "            df['citation_date'] = 1422835200\n",
        "        elif col == 'metrics':\n",
        "            df['metrics'] = [{'citations': {'total': 0}}] * len(df)\n",
        "\n",
        "    print(\"Added dummy values for missing columns to allow processing to continue\")\n"
      ],
      "metadata": {
        "id": "sCJBS50Mrq1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import pytz\n",
        "from datetime import datetime\n",
        "\n",
        "###########################################\n",
        "# Fix tweet text extraction after Step 1\n",
        "###########################################\n",
        "\n",
        "# If the loaded DataFrame (df) has a 'tweet' column (with nested tweet data),\n",
        "# extract the tweet text from it. Otherwise, assume df already has a 'text' column.\n",
        "if 'tweet' in df.columns:\n",
        "    # Create a new column 'text' from df['tweet']['text']\n",
        "    def extract_text(nested):\n",
        "        if isinstance(nested, dict):\n",
        "            return nested.get('text', '')\n",
        "        return ''\n",
        "    df['text'] = df['tweet'].apply(extract_text)\n",
        "    print(\"Extracted tweet text from the nested 'tweet' field.\")\n",
        "else:\n",
        "    print(\"Using existing 'text' column in df.\")\n",
        "\n",
        "# Quick debug print: show first 10 tweet texts\n",
        "print(\"\\n=== Debug: Raw 'text' samples ===\")\n",
        "print(df['text'].head(10))\n",
        "\n",
        "###########################################\n",
        "# Preprocessing Functions (Step 2)\n",
        "###########################################\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean tweet text by removing URLs, mentions, hashtags, and special characters.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+', '', text)        # Remove URLs\n",
        "    text = re.sub(r'@\\S+', '', text)             # Remove mentions\n",
        "    text = re.sub(r'#\\S+', '', text)             # Remove hashtags (or use text.replace('#','') to keep the word)\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)         # Remove special characters\n",
        "    text = re.sub(r'\\d+', '', text)              # Remove numbers\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()     # Remove extra whitespace\n",
        "    return text\n",
        "\n",
        "def extract_hashtags(text):\n",
        "    \"\"\"\n",
        "    Extract hashtags from tweet text.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    hashtags = re.findall(r'#(\\w+)', text.lower())\n",
        "    return hashtags\n",
        "\n",
        "def preprocess_tweets(df):\n",
        "    \"\"\"\n",
        "    Preprocess the tweet dataset by:\n",
        "      1. Converting UNIX timestamps to datetime\n",
        "      2. Identifying team affiliation based on hashtags and text content\n",
        "      3. Cleaning tweet text\n",
        "      4. Extracting additional metadata\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Preprocessing Tweets ---\")\n",
        "    processed_df = df.copy()\n",
        "\n",
        "    # Extract hashtags (before cleaning text)\n",
        "    print(\"Extracting hashtags...\")\n",
        "    processed_df['hashtags'] = processed_df['text'].apply(extract_hashtags)\n",
        "\n",
        "    # Clean tweet text\n",
        "    print(\"Cleaning tweet text...\")\n",
        "    processed_df['clean_text'] = processed_df['text'].apply(clean_text)\n",
        "\n",
        "    # Convert UNIX timestamp to PST datetime (using 'citation_date')\n",
        "    print(\"Converting timestamps...\")\n",
        "    pst_tz = pytz.timezone('America/Los_Angeles')\n",
        "    def convert_timestamp(ts):\n",
        "        if pd.isnull(ts):\n",
        "            return None\n",
        "        try:\n",
        "            return datetime.fromtimestamp(float(ts), pst_tz)\n",
        "        except (ValueError, TypeError, OverflowError):\n",
        "            return None\n",
        "    processed_df['datetime'] = processed_df['citation_date'].apply(convert_timestamp)\n",
        "\n",
        "    # Remove rows with invalid dates\n",
        "    valid_date_mask = ~processed_df['datetime'].isna()\n",
        "    if valid_date_mask.sum() < len(processed_df):\n",
        "        print(f\"Warning: {len(processed_df) - valid_date_mask.sum()} tweets have invalid dates and will be removed.\")\n",
        "        processed_df = processed_df[valid_date_mask].copy()\n",
        "\n",
        "    # Create time features\n",
        "    processed_df['hour'] = processed_df['datetime'].dt.hour\n",
        "    processed_df['minute'] = processed_df['datetime'].dt.minute\n",
        "\n",
        "    # Define game time (example for Super Bowl 49)\n",
        "    game_start = datetime(2015, 2, 1, 15, 30, tzinfo=pst_tz)\n",
        "    game_end   = datetime(2015, 2, 1, 19, 0, tzinfo=pst_tz)\n",
        "\n",
        "    processed_df['time_period'] = 'unknown'\n",
        "    processed_df.loc[processed_df['datetime'] < game_start, 'time_period'] = 'pre_game'\n",
        "    processed_df.loc[(processed_df['datetime'] >= game_start) & (processed_df['datetime'] <= game_end), 'time_period'] = 'during_game'\n",
        "    processed_df.loc[processed_df['datetime'] > game_end, 'time_period'] = 'post_game'\n",
        "\n",
        "    # Approximate game quarters\n",
        "    q1_end = datetime(2015, 2, 1, 16, 15, tzinfo=pst_tz)\n",
        "    q2_end = datetime(2015, 2, 1, 17, 0, tzinfo=pst_tz)\n",
        "    q3_end = datetime(2015, 2, 1, 17, 45, tzinfo=pst_tz)\n",
        "    processed_df['quarter'] = 'not_during_game'\n",
        "    processed_df.loc[(processed_df['datetime'] >= game_start) & (processed_df['datetime'] < q1_end), 'quarter'] = 'q1'\n",
        "    processed_df.loc[(processed_df['datetime'] >= q1_end) & (processed_df['datetime'] < q2_end), 'quarter'] = 'q2'\n",
        "    processed_df.loc[(processed_df['datetime'] >= q2_end) & (processed_df['datetime'] < q3_end), 'quarter'] = 'q3'\n",
        "    processed_df.loc[(processed_df['datetime'] >= q3_end) & (processed_df['datetime'] <= game_end), 'quarter'] = 'q4'\n",
        "\n",
        "    # Identify team affiliations based on hashtags and text content\n",
        "    print(\"Identifying team affiliations...\")\n",
        "    processed_df['team'] = 'neutral'\n",
        "\n",
        "    # Define patterns (adjust these as needed)\n",
        "    patriots_patterns = ['gopatriots', 'patriots', 'pats', 'tombrady', 'gopats', 'patriotsnation', 'patsnation']\n",
        "    seahawks_patterns = ['gohawks', 'seahawks', 'seattleseahawks', 'goseahawks', '12s', '12thman', 'hawksnation']\n",
        "\n",
        "    # Create masks based on text content\n",
        "    patriots_mask = processed_df['text'].str.lower().str.contains('|'.join(patriots_patterns), regex=True)\n",
        "    seahawks_mask = processed_df['text'].str.lower().str.contains('|'.join(seahawks_patterns), regex=True)\n",
        "    print(f\"Number of tweets matching patriots text pattern: {patriots_mask.sum()}\")\n",
        "    print(f\"Number of tweets matching seahawks text pattern: {seahawks_mask.sum()}\")\n",
        "\n",
        "    # Create masks based on hashtags\n",
        "    hashtags_pats = processed_df['hashtags'].apply(lambda tags: any(pat in tags for pat in patriots_patterns))\n",
        "    hashtags_hawks = processed_df['hashtags'].apply(lambda tags: any(pat in tags for pat in seahawks_patterns))\n",
        "    print(f\"Number of tweets matching patriots hashtag: {hashtags_pats.sum()}\")\n",
        "    print(f\"Number of tweets matching seahawks hashtag: {hashtags_hawks.sum()}\")\n",
        "\n",
        "    # Assign teams\n",
        "    processed_df.loc[patriots_mask | hashtags_pats, 'team'] = 'patriots'\n",
        "    seahawks_unassigned = (processed_df['team'] == 'neutral') & (seahawks_mask | hashtags_hawks)\n",
        "    processed_df.loc[seahawks_unassigned, 'team'] = 'seahawks'\n",
        "\n",
        "    # Extract metadata\n",
        "    print(\"Extracting tweet metadata...\")\n",
        "    def extract_followers(author_obj):\n",
        "        if not isinstance(author_obj, dict):\n",
        "            return 0\n",
        "        return author_obj.get('followers', 0) or 0\n",
        "    def extract_retweets(metrics_obj):\n",
        "        if not isinstance(metrics_obj, dict):\n",
        "            return 0\n",
        "        citations = metrics_obj.get('citations', {})\n",
        "        if not isinstance(citations, dict):\n",
        "            return 0\n",
        "        return citations.get('total', 0) or 0\n",
        "    processed_df['follower_count'] = processed_df['author'].apply(extract_followers)\n",
        "    processed_df['retweet_count'] = processed_df['metrics'].apply(extract_retweets)\n",
        "\n",
        "    # Filter for team-affiliated tweets only\n",
        "    team_tweets = processed_df[processed_df['team'] != 'neutral'].copy()\n",
        "    print(f\"Found {len(team_tweets)} tweets with team affiliations\")\n",
        "    print(\"\\nTeam distribution:\")\n",
        "    print(team_tweets['team'].value_counts())\n",
        "\n",
        "    if len(team_tweets) < 100:\n",
        "        print(\"\\nWARNING: Very few team-affiliated tweets found. This might affect model quality.\")\n",
        "        top_hs = processed_df['hashtags'].explode().value_counts().head(20)\n",
        "        print(\"\\nTop hashtags after cleaning/extraction:\")\n",
        "        print(top_hs)\n",
        "\n",
        "    return team_tweets\n",
        "\n",
        "###########################################\n",
        "# Run Preprocessing\n",
        "###########################################\n",
        "team_tweets = preprocess_tweets(df)\n",
        "\n",
        "print(\"\\n--- Final Preprocessed Data Statistics ---\")\n",
        "print(f\"Number of Patriots tweets: {(team_tweets['team'] == 'patriots').sum()}\")\n",
        "print(f\"Number of Seahawks tweets: {(team_tweets['team'] == 'seahawks').sum()}\")\n",
        "\n",
        "print(\"\\nTweets by time period:\")\n",
        "print(team_tweets['time_period'].value_counts())\n",
        "\n",
        "print(\"\\nTweets by quarter:\")\n",
        "print(team_tweets['quarter'].value_counts())\n",
        "\n",
        "print(\"\\nSample of cleaned tweets:\")\n",
        "for team in ['patriots', 'seahawks']:\n",
        "    subset = team_tweets[team_tweets['team'] == team]\n",
        "    if len(subset) > 0:\n",
        "        sample_size = min(3, len(subset))\n",
        "        tweets = subset['clean_text'].sample(sample_size).tolist()\n",
        "        print(f\"\\n{team.upper()} fan tweets:\")\n",
        "        for t in tweets:\n",
        "            print(f\"- {t}\")\n"
      ],
      "metadata": {
        "id": "sjeRZ5Mh7Ob9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "## Step 3: Feature Engineering\n",
        "\"\"\"\n",
        "\n",
        "print(\"team_tweets shape:\", team_tweets.shape)\n",
        "print(\"Number of Patriots tweets:\", (team_tweets['team'] == 'patriots').sum())\n",
        "print(\"Number of Seahawks tweets:\", (team_tweets['team'] == 'seahawks').sum())\n",
        "print(\"Number of empty clean_text:\", (team_tweets['clean_text'].str.strip() == '').sum())\n",
        "\n",
        "\n",
        "def engineer_features(team_tweets):\n",
        "    \"\"\"\n",
        "    Engineer features for the classification task:\n",
        "    1. TF-IDF features from cleaned text\n",
        "    2. Additional metadata features\n",
        "\n",
        "    Parameters:\n",
        "    - team_tweets: DataFrame containing preprocessed tweets\n",
        "\n",
        "    Returns:\n",
        "    - X: Feature matrix\n",
        "    - y: Target labels\n",
        "    - feature_names: Names of the features\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Feature Engineering ---\")\n",
        "\n",
        "    # Ensure we have required columns\n",
        "    required_cols = ['clean_text', 'team', 'time_period', 'quarter', 'follower_count', 'retweet_count']\n",
        "    missing_cols = [col for col in required_cols if col not in team_tweets.columns]\n",
        "\n",
        "    if missing_cols:\n",
        "        print(f\"Warning: Missing columns for feature engineering: {missing_cols}\")\n",
        "        # Try to fix or skip these features\n",
        "\n",
        "    # Remove rows with empty cleaned text\n",
        "    team_tweets = team_tweets[team_tweets['clean_text'].str.len() > 0].copy()\n",
        "\n",
        "    # Create target variable: 1 for Patriots, 0 for Seahawks\n",
        "    y = (team_tweets['team'] == 'patriots').astype(int)\n",
        "\n",
        "    # Before vectorizing, ensure team_tweets isn't empty:\n",
        "    if team_tweets.empty:\n",
        "        print(\"No data left after filtering. Returning None.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    # Check how many docs remain:\n",
        "    print(f\"Number of tweets: {len(team_tweets)}\")\n",
        "\n",
        "    # If you're debugging, print out some of the text:\n",
        "    print(\"Sample cleaned text:\\n\", team_tweets['clean_text'].head(10).tolist())\n",
        "\n",
        "    # Use TF-IDF vectorizer for text features\n",
        "    print(\"Creating TF-IDF features...\")\n",
        "    # We limit to 2000 features to keep the model manageable\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        max_features=2000,\n",
        "        min_df=5,         # Minimum document frequency\n",
        "        max_df=0.9,       # Maximum document frequency (ignore too common words)\n",
        "        stop_words=stop_words  # Remove English stopwords\n",
        "    )\n",
        "\n",
        "    X_text = vectorizer.fit_transform(team_tweets['clean_text'])\n",
        "    text_feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    print(f\"Created {X_text.shape[1]} text features\")\n",
        "\n",
        "    # Create metadata features\n",
        "    print(\"Creating metadata features...\")\n",
        "    from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "    # Start with text features\n",
        "    feature_matrices = [X_text]\n",
        "    feature_names = list(text_feature_names)\n",
        "\n",
        "    # Time-related features (encoded as dummy variables)\n",
        "    if 'time_period' in team_tweets.columns:\n",
        "        time_features = pd.get_dummies(team_tweets['time_period'], prefix='time')\n",
        "        feature_matrices.append(csr_matrix(time_features.values))\n",
        "        feature_names.extend(time_features.columns.tolist())\n",
        "\n",
        "    if 'quarter' in team_tweets.columns:\n",
        "        quarter_features = pd.get_dummies(team_tweets['quarter'], prefix='quarter')\n",
        "        feature_matrices.append(csr_matrix(quarter_features.values))\n",
        "        feature_names.extend(quarter_features.columns.tolist())\n",
        "\n",
        "    # Log-transform follower and retweet counts (to handle skewed distributions)\n",
        "    if 'follower_count' in team_tweets.columns:\n",
        "        log_followers = np.log1p(team_tweets['follower_count'].values).reshape(-1, 1)\n",
        "        feature_matrices.append(csr_matrix(log_followers))\n",
        "        feature_names.append('log_followers')\n",
        "\n",
        "    if 'retweet_count' in team_tweets.columns:\n",
        "        log_retweets = np.log1p(team_tweets['retweet_count'].values).reshape(-1, 1)\n",
        "        feature_matrices.append(csr_matrix(log_retweets))\n",
        "        feature_names.append('log_retweets')\n",
        "\n",
        "    # Combine all features\n",
        "    print(\"Combining features...\")\n",
        "    X = hstack(feature_matrices)\n",
        "\n",
        "    print(f\"Final feature matrix shape: {X.shape}\")\n",
        "\n",
        "    return X, y, feature_names, team_tweets.index\n",
        "\n",
        "# Engineer features\n",
        "X, y, feature_names, indices = engineer_features(team_tweets)\n",
        "\n",
        "if y is None:\n",
        "    print(\"\\nNo data to display class distribution.\")\n",
        "else:\n",
        "    print(\"\\nClass distribution:\")\n",
        "    print(f\"Patriots tweets: {sum(y == 1)}\")\n",
        "    print(f\"Seahawks tweets: {sum(y == 0)}\")\n",
        "    ratio = sum(y == 1) / (sum(y == 0) or 1)\n",
        "    print(f\"Ratio (Patriots:Seahawks): {ratio:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "rw1-ZITztSBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "## Step 4: Model Training with Multiple Baselines\n",
        "\"\"\"\n",
        "\n",
        "def train_and_evaluate_models(X, y, feature_names):\n",
        "    \"\"\"\n",
        "    Train and evaluate multiple models to establish baselines and find the best performer.\n",
        "\n",
        "    Parameters:\n",
        "    - X: Feature matrix\n",
        "    - y: Target labels\n",
        "    - feature_names: Names of the features\n",
        "\n",
        "    Returns:\n",
        "    - Dictionary containing trained models and their performance metrics\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Model Training and Evaluation ---\")\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "    print(f\"Testing set: {X_test.shape[0]} samples\")\n",
        "\n",
        "    # Handle class imbalance using random oversampling\n",
        "    oversampler = RandomOverSampler(random_state=42)\n",
        "    X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
        "\n",
        "    print(f\"After oversampling - Training set: {X_train_resampled.shape[0]} samples\")\n",
        "    print(f\"Class distribution after oversampling: {np.bincount(y_train_resampled)}\")\n",
        "\n",
        "    # Define models to train\n",
        "    models = {\n",
        "        'Random Baseline': DummyClassifier(strategy='uniform', random_state=42),\n",
        "        'Most Frequent Baseline': DummyClassifier(strategy='most_frequent'),\n",
        "        'Naive Bayes': MultinomialNB(),\n",
        "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42, C=1.0),\n",
        "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10, n_jobs=-1)\n",
        "    }\n",
        "\n",
        "    # Train and evaluate each model\n",
        "    results = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nTraining {name}...\")\n",
        "        model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "        # Evaluate on test set\n",
        "        y_pred = model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        report = classification_report(y_test, y_pred, target_names=['Seahawks', 'Patriots'])\n",
        "\n",
        "        print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
        "        print(report)\n",
        "\n",
        "        # Store results\n",
        "        results[name] = {\n",
        "            'model': model,\n",
        "            'accuracy': accuracy,\n",
        "            'predictions': y_pred,\n",
        "            'report': report\n",
        "        }\n",
        "\n",
        "        # For models that support feature importance, extract and display\n",
        "        if name == 'Logistic Regression':\n",
        "            # Get coefficients and corresponding feature names\n",
        "            coef = model.coef_[0]\n",
        "            feature_importance = pd.DataFrame({\n",
        "                'Feature': feature_names,\n",
        "                'Importance': coef\n",
        "            })\n",
        "\n",
        "            # Sort by absolute importance\n",
        "            feature_importance['Abs_Importance'] = abs(feature_importance['Importance'])\n",
        "            feature_importance = feature_importance.sort_values('Abs_Importance', ascending=False)\n",
        "\n",
        "            # Display top features for each class\n",
        "            print(\"\\nTop features for Patriots (positive coefficients):\")\n",
        "            print(feature_importance[feature_importance['Importance'] > 0].head(10))\n",
        "\n",
        "            print(\"\\nTop features for Seahawks (negative coefficients):\")\n",
        "            print(feature_importance[feature_importance['Importance'] < 0].head(10))\n",
        "\n",
        "            # Store feature importance\n",
        "            results[name]['feature_importance'] = feature_importance\n",
        "\n",
        "        elif name == 'Random Forest':\n",
        "            # Get feature importance\n",
        "            importances = model.feature_importances_\n",
        "            feature_importance = pd.DataFrame({\n",
        "                'Feature': feature_names,\n",
        "                'Importance': importances\n",
        "            }).sort_values('Importance', ascending=False)\n",
        "\n",
        "            print(\"\\nTop Random Forest feature importances:\")\n",
        "            print(feature_importance.head(20))\n",
        "\n",
        "            # Store feature importance\n",
        "            results[name]['feature_importance'] = feature_importance\n",
        "\n",
        "    return results\n",
        "\n",
        "# Train and evaluate models\n",
        "model_results = train_and_evaluate_models(X, y, feature_names)\n"
      ],
      "metadata": {
        "id": "WWv0NjTm9OeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pytz\n",
        "from datetime import datetime\n",
        "\n",
        "###########################################\n",
        "# STEP 5 (Time-Based Analysis) with Debug\n",
        "###########################################\n",
        "\n",
        "def analyze_time_patterns(team_tweets):\n",
        "    \"\"\"\n",
        "    Analyze how fan behavior changes throughout the game.\n",
        "\n",
        "    Parameters:\n",
        "    - team_tweets: DataFrame containing preprocessed tweets\n",
        "\n",
        "    Returns:\n",
        "    - None (generates plots and prints analysis)\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Time-Based Analysis ---\")\n",
        "\n",
        "    # Debug: check input size\n",
        "    print(f\"Initial team_tweets shape: {team_tweets.shape}\")\n",
        "    print(\"Columns:\", list(team_tweets.columns))\n",
        "\n",
        "    # Ensure 'datetime' is a proper datetime dtype\n",
        "    if not pd.api.types.is_datetime64_any_dtype(team_tweets['datetime']):\n",
        "        print(\"Converting 'datetime' column to datetime dtype...\")\n",
        "        team_tweets['datetime'] = pd.to_datetime(team_tweets['datetime'])\n",
        "    else:\n",
        "        print(\"'datetime' column is already a datetime dtype.\")\n",
        "\n",
        "    # Convert datetime to more manageable 5-minute intervals\n",
        "    print(\"Creating 'time_bin' column (5-minute intervals)...\")\n",
        "    team_tweets['time_bin'] = team_tweets['datetime'].dt.floor('5min')\n",
        "    print(\"time_bin head:\\n\", team_tweets['time_bin'].head())\n",
        "\n",
        "    # Group by time bin and team, count tweets\n",
        "    print(\"Grouping by (time_bin, team) to count tweets...\")\n",
        "    time_counts = team_tweets.groupby(['time_bin', 'team']).size().unstack(fill_value=0)\n",
        "\n",
        "    # Debug: see a few rows of time_counts\n",
        "    print(\"\\ntime_counts head (showing first 10 rows):\\n\", time_counts.head(10))\n",
        "\n",
        "    # Ensure columns for both teams exist\n",
        "    if 'patriots' not in time_counts.columns:\n",
        "        time_counts['patriots'] = 0\n",
        "    if 'seahawks' not in time_counts.columns:\n",
        "        time_counts['seahawks'] = 0\n",
        "\n",
        "    # Plot tweet volume over time by team\n",
        "    print(\"Plotting tweet volume over time by team...\")\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    time_counts['patriots'].plot(label='Patriots', color='blue')\n",
        "    time_counts['seahawks'].plot(label='Seahawks', color='green')\n",
        "    plt.title('Tweet Volume by Team Throughout Super Bowl Sunday')\n",
        "    plt.xlabel('Time (PST)')\n",
        "    plt.ylabel('Number of Tweets per 5 Minutes')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add game period markers\n",
        "    game_start = datetime(2015, 2, 1, 15, 30, tzinfo=pytz.timezone('America/Los_Angeles'))\n",
        "    game_end   = datetime(2015, 2, 1, 19, 0, tzinfo=pytz.timezone('America/Los_Angeles'))\n",
        "\n",
        "    plt.axvline(x=game_start, color='red', linestyle='--', alpha=0.7, label='Game Start')\n",
        "    plt.axvline(x=game_end, color='red', linestyle='--', alpha=0.7, label='Game End')\n",
        "    plt.savefig('tweet_volume_by_team.png')\n",
        "    plt.show()\n",
        "    print(\"Created tweet volume plot: tweet_volume_by_team.png\")\n",
        "\n",
        "    # Analyze engagement metrics over time\n",
        "    print(\"\\nAnalyzing engagement metrics...\")\n",
        "    team_tweets['engagement_ratio'] = team_tweets['retweet_count'] / (team_tweets['follower_count'] + 1)\n",
        "\n",
        "    # Group by quarter and team for average engagement\n",
        "    print(\"Grouping by (quarter, team) to compute mean engagement_ratio...\")\n",
        "    quarter_engagement = team_tweets.groupby(['quarter', 'team'])['engagement_ratio'].mean().unstack()\n",
        "\n",
        "    print(\"\\nquarter_engagement DataFrame:\\n\", quarter_engagement)\n",
        "\n",
        "    # Plot engagement by quarter\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    quarter_engagement.plot(kind='bar', figsize=(12, 6))\n",
        "    plt.title('Average Engagement Ratio by Quarter and Team')\n",
        "    plt.xlabel('Game Quarter')\n",
        "    plt.ylabel('Average Engagement Ratio (Retweets / Followers)')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('engagement_by_quarter.png')\n",
        "    plt.show()\n",
        "    print(\"Created engagement analysis plot: engagement_by_quarter.png\")\n",
        "\n",
        "    # Compute ratio of patriots to seahawks tweets\n",
        "    print(\"\\nComputing ratio of patriots to seahawks tweets per 5-min bin...\")\n",
        "    time_counts['ratio'] = time_counts['patriots'] / (time_counts['seahawks'] + 1)\n",
        "\n",
        "    # Debug: see first 10 ratio values\n",
        "    print(\"\\nFirst 10 ratio values:\\n\", time_counts['ratio'].head(10))\n",
        "\n",
        "    # Option A: Filter out bins with very few total tweets\n",
        "    time_counts['total'] = time_counts['patriots'] + time_counts['seahawks']\n",
        "    filtered = time_counts[time_counts['total'] >= 5].copy()\n",
        "    filtered['ratio'] = filtered['patriots'] / (filtered['seahawks'] + 1)\n",
        "\n",
        "    # Identify key moments with ratio > 1.5 or ratio < 0.67\n",
        "    key_moments = filtered[\n",
        "        (filtered['ratio'] > 1.5) | (filtered['ratio'] < 0.67)\n",
        "    ].index.tolist()\n",
        "    key_moments.sort()\n",
        "\n",
        "    print(\"\\n--- Approach 1: Filtered bins (total >= 5) ---\")\n",
        "    print(f\"Found {len(key_moments)} key moments with significant difference (ratio>1.5 or ratio<0.67)\")\n",
        "\n",
        "    if key_moments:\n",
        "        print(\"Showing up to 10 such moments:\")\n",
        "        for moment in key_moments[:10]:\n",
        "            ratio_val = filtered.loc[moment, 'ratio']\n",
        "            # Decide who is more active\n",
        "            if ratio_val == 0:\n",
        "                more_active = \"Seahawks (no Patriots tweets)\"\n",
        "            elif np.isinf(ratio_val):\n",
        "                more_active = \"Patriots (no Seahawks tweets)\"\n",
        "            else:\n",
        "                more_active = \"Patriots\" if ratio_val > 1 else \"Seahawks\"\n",
        "            factor = max(ratio_val, 1/ratio_val) if ratio_val != 0 else np.inf\n",
        "            print(f\"{moment} - ratio={ratio_val:.3f} => {more_active} were {factor:.2f}x more active\")\n",
        "\n",
        "    # Option B: Keep all bins, but handle infinite or zero ratios explicitly\n",
        "    all_time_counts = time_counts.copy()\n",
        "    all_time_counts['ratio'] = all_time_counts['patriots'] / (all_time_counts['seahawks'] + 1)\n",
        "    all_key_moments = all_time_counts[\n",
        "        (all_time_counts['ratio'] > 1.5) | (all_time_counts['ratio'] < 0.67)\n",
        "    ].index.tolist()\n",
        "    all_key_moments.sort()\n",
        "\n",
        "    print(\"\\n--- Approach 2: All bins (including total < 5) ---\")\n",
        "    print(f\"Found {len(all_key_moments)} key moments with significant difference in ratio\")\n",
        "    if all_key_moments:\n",
        "        print(\"Showing up to 10 such moments:\")\n",
        "        for moment in all_key_moments[:10]:\n",
        "            ratio_val = all_time_counts.loc[moment, 'ratio']\n",
        "            if ratio_val == 0:\n",
        "                # Means patriots=0, seahawks>0\n",
        "                print(f\"{moment}: Seahawks fans were MUCH more active (no Patriots tweets).\")\n",
        "            elif np.isinf(ratio_val):\n",
        "                # Means seahawks=0, patriots>0\n",
        "                print(f\"{moment}: Patriots fans were MUCH more active (no Seahawks tweets).\")\n",
        "            else:\n",
        "                more_active = \"Patriots\" if ratio_val > 1 else \"Seahawks\"\n",
        "                factor = max(ratio_val, 1/ratio_val)\n",
        "                print(f\"{moment}: {more_active} fans were {factor:.2f}x more active\")\n",
        "\n",
        "# Example usage (assuming you have a DataFrame called team_tweets)\n",
        "analyze_time_patterns(team_tweets)\n"
      ],
      "metadata": {
        "id": "r8k5MZo1_BNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "###########################################\n",
        "# Step 6: Keyword Analysis by Game Period\n",
        "###########################################\n",
        "\n",
        "def analyze_keywords_by_period(team_tweets):\n",
        "    \"\"\"\n",
        "    Analyze how keywords used by fans change throughout the game.\n",
        "\n",
        "    Parameters:\n",
        "    - team_tweets: DataFrame containing preprocessed tweets\n",
        "\n",
        "    Returns:\n",
        "    - None (prints analysis and generates plots)\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Keyword Analysis by Game Period ---\")\n",
        "\n",
        "    # Define periods of interest\n",
        "    periods = ['pre_game', 'during_game', 'post_game']\n",
        "    period_keywords = {}\n",
        "\n",
        "    for period in periods:\n",
        "        print(f\"\\nProcessing period: {period}\")\n",
        "        # Filter tweets by period\n",
        "        period_data = team_tweets[team_tweets['time_period'] == period]\n",
        "        print(f\"Total tweets in {period}: {len(period_data)}\")\n",
        "\n",
        "        # If not enough data, skip this period\n",
        "        if len(period_data) < 10:\n",
        "            print(f\"Skipping {period} period due to insufficient tweets (<10).\")\n",
        "            continue\n",
        "\n",
        "        # Separate tweets by team\n",
        "        patriots_tweets = period_data[period_data['team'] == 'patriots']['clean_text']\n",
        "        seahawks_tweets = period_data[period_data['team'] == 'seahawks']['clean_text']\n",
        "\n",
        "        print(f\"Patriots tweets count in {period}: {len(patriots_tweets)}\")\n",
        "        print(f\"Seahawks tweets count in {period}: {len(seahawks_tweets)}\")\n",
        "\n",
        "        # Create CountVectorizer with lower min_df (e.g., 2) for small datasets\n",
        "        vectorizer_params = {\n",
        "            'max_features': 100,\n",
        "            'min_df': 2,\n",
        "            'stop_words': stop_words  # ensure stop_words is defined in your environment\n",
        "        }\n",
        "\n",
        "        # Process Patriots tweets\n",
        "        patriots_term_counts = []\n",
        "        if len(patriots_tweets) > 10:\n",
        "            print(\"\\nAnalyzing keywords for Patriots fans...\")\n",
        "            try:\n",
        "                vectorizer_pats = CountVectorizer(**vectorizer_params)\n",
        "                patriots_dtm = vectorizer_pats.fit_transform(patriots_tweets)\n",
        "                patriots_terms = vectorizer_pats.get_feature_names_out()\n",
        "                patriots_counts = patriots_dtm.sum(axis=0).A1\n",
        "                patriots_term_counts = sorted(zip(patriots_terms, patriots_counts), key=lambda x: x[1], reverse=True)\n",
        "                print(f\"Top terms for Patriots fans ({period}):\")\n",
        "                for term, count in patriots_term_counts[:15]:\n",
        "                    print(f\"- {term}: {count}\")\n",
        "            except ValueError as e:\n",
        "                print(f\"Error processing Patriots tweets in {period}: {e}\")\n",
        "        else:\n",
        "            print(f\"Not enough Patriots tweets in {period} to analyze.\")\n",
        "\n",
        "        # Process Seahawks tweets\n",
        "        seahawks_term_counts = []\n",
        "        if len(seahawks_tweets) > 10:\n",
        "            print(\"\\nAnalyzing keywords for Seahawks fans...\")\n",
        "            try:\n",
        "                vectorizer_hawks = CountVectorizer(**vectorizer_params)\n",
        "                seahawks_dtm = vectorizer_hawks.fit_transform(seahawks_tweets)\n",
        "                seahawks_terms = vectorizer_hawks.get_feature_names_out()\n",
        "                seahawks_counts = seahawks_dtm.sum(axis=0).A1\n",
        "                seahawks_term_counts = sorted(zip(seahawks_terms, seahawks_counts), key=lambda x: x[1], reverse=True)\n",
        "                print(f\"Top terms for Seahawks fans ({period}):\")\n",
        "                for term, count in seahawks_term_counts[:15]:\n",
        "                    print(f\"- {term}: {count}\")\n",
        "            except ValueError as e:\n",
        "                print(f\"Error processing Seahawks tweets in {period}: {e}\")\n",
        "        else:\n",
        "            print(f\"Not enough Seahawks tweets in {period} to analyze.\")\n",
        "\n",
        "        period_keywords[period] = {\n",
        "            'patriots': patriots_term_counts[:20] if len(patriots_tweets) > 10 else [],\n",
        "            'seahawks': seahawks_term_counts[:20] if len(seahawks_tweets) > 10 else []\n",
        "        }\n",
        "\n",
        "    # Create visualizations for keyword differences for each period with sufficient data\n",
        "    for period in period_keywords:\n",
        "        if not period_keywords[period]['patriots'] or not period_keywords[period]['seahawks']:\n",
        "            print(f\"Skipping visualization for {period} due to insufficient keyword data.\")\n",
        "            continue\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        # Patriots keywords bar chart\n",
        "        plt.subplot(1, 2, 1)\n",
        "        patriots_words = [word for word, count in period_keywords[period]['patriots']][:10]\n",
        "        patriots_counts = [count for word, count in period_keywords[period]['patriots']][:10]\n",
        "        y_pos = np.arange(len(patriots_words))\n",
        "        plt.barh(y_pos, patriots_counts, color='blue')\n",
        "        plt.yticks(y_pos, patriots_words)\n",
        "        plt.title(f'Patriots Fan Keywords ({period})')\n",
        "        plt.xlabel('Count')\n",
        "\n",
        "        # Seahawks keywords bar chart\n",
        "        plt.subplot(1, 2, 2)\n",
        "        seahawks_words = [word for word, count in period_keywords[period]['seahawks']][:10]\n",
        "        seahawks_counts = [count for word, count in period_keywords[period]['seahawks']][:10]\n",
        "        y_pos = np.arange(len(seahawks_words))\n",
        "        plt.barh(y_pos, seahawks_counts, color='green')\n",
        "        plt.yticks(y_pos, seahawks_words)\n",
        "        plt.title(f'Seahawks Fan Keywords ({period})')\n",
        "        plt.xlabel('Count')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        filename = f'keywords_{period}.png'\n",
        "        plt.savefig(filename)\n",
        "        plt.show()\n",
        "        print(f\"Created keyword visualization: {filename}\")\n",
        "\n",
        "    return period_keywords\n",
        "\n",
        "# Run the keyword analysis function\n",
        "keyword_results = analyze_keywords_by_period(team_tweets)\n"
      ],
      "metadata": {
        "id": "Zc8YmGXQBKN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "## Step 7: Create Game Timeline Visualization\n",
        "\"\"\"\n",
        "\n",
        "def create_game_timeline_visualization(team_tweets):\n",
        "    \"\"\"\n",
        "    Create a timeline visualization showing key moments in the game and corresponding tweet activity.\n",
        "\n",
        "    Parameters:\n",
        "    - team_tweets: DataFrame containing preprocessed tweets\n",
        "\n",
        "    Returns:\n",
        "    - None (generates plot)\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"                 GAME TIMELINE VISUALIZATION                        \")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Define key game events for Super Bowl 49\n",
        "    key_events = [\n",
        "        {'time': datetime(2015, 2, 1, 15, 30, tzinfo=pytz.timezone('America/Los_Angeles')),\n",
        "         'event': 'Kickoff', 'team': 'neutral'},\n",
        "        {'time': datetime(2015, 2, 1, 15, 45, tzinfo=pytz.timezone('America/Los_Angeles')),\n",
        "         'event': 'Patriots TD - Brady to LaFell', 'team': 'patriots'},\n",
        "        {'time': datetime(2015, 2, 1, 15, 55, tzinfo=pytz.timezone('America/Los_Angeles')),\n",
        "         'event': 'Seahawks TD - Lynch 3-yard run', 'team': 'seahawks'},\n",
        "        {'time': datetime(2015, 2, 1, 16, 8, tzinfo=pytz.timezone('America/Los_Angeles')),\n",
        "         'event': 'Patriots TD - Brady to Gronkowski', 'team': 'patriots'},\n",
        "        {'time': datetime(2015, 2, 1, 16, 15, tzinfo=pytz.timezone('America/Los_Angeles')),\n",
        "         'event': 'Seahawks TD - Wilson to Matthews', 'team': 'seahawks'},\n",
        "        {'time': datetime(2015, 2, 1, 16, 35, tzinfo=pytz.timezone('America/Los_Angeles')),\n",
        "         'event': 'Halftime Show', 'team': 'neutral'},\n",
        "        {'time': datetime(2015, 2, 1, 17, 10, tzinfo=pytz.timezone('America/Los_Angeles')),\n",
        "         'event': 'Seahawks TD - Baldwin from Wilson', 'team': 'seahawks'},\n",
        "        {'time': datetime(2015, 2, 1, 18, 0, tzinfo=pytz.timezone('America/Los_Angeles')),\n",
        "         'event': 'Patriots TD - Brady to Amendola', 'team': 'patriots'},\n",
        "        {'time': datetime(2015, 2, 1, 18, 10, tzinfo=pytz.timezone('America/Los_Angeles')),\n",
        "         'event': 'Patriots TD - Brady to Edelman', 'team': 'patriots'},\n",
        "        {'time': datetime(2015, 2, 1, 18, 55, tzinfo=pytz.timezone('America/Los_Angeles')),\n",
        "         'event': 'Butler Interception', 'team': 'patriots'},\n",
        "        {'time': datetime(2015, 2, 1, 19, 0, tzinfo=pytz.timezone('America/Los_Angeles')),\n",
        "         'event': 'Game End - Patriots Win', 'team': 'patriots'}\n",
        "    ]\n",
        "\n",
        "    # Create a DataFrame for events\n",
        "    events_df = pd.DataFrame(key_events)\n",
        "\n",
        "    # Group tweets by 2-minute bins and team\n",
        "    team_tweets['time_bin'] = team_tweets['datetime'].dt.floor('2min')\n",
        "    time_counts = team_tweets.groupby(['time_bin', 'team']).size().unstack(fill_value=0)\n",
        "\n",
        "    # Ensure both teams are in columns\n",
        "    if 'patriots' not in time_counts.columns:\n",
        "        time_counts['patriots'] = 0\n",
        "    if 'seahawks' not in time_counts.columns:\n",
        "        time_counts['seahawks'] = 0\n",
        "\n",
        "    # Calculate total tweet volume\n",
        "    time_counts['total'] = time_counts['patriots'] + time_counts['seahawks']\n",
        "\n",
        "    # Plot the game timeline\n",
        "    plt.figure(figsize=(20, 10))\n",
        "\n",
        "    # Plot tweet volume\n",
        "    plt.plot(time_counts.index, time_counts['patriots'], label='Patriots Tweets', color='blue', alpha=0.7)\n",
        "    plt.plot(time_counts.index, time_counts['seahawks'], label='Seahawks Tweets', color='green', alpha=0.7)\n",
        "\n",
        "    # Add event markers\n",
        "    for _, event in events_df.iterrows():\n",
        "        color = 'blue' if event['team'] == 'patriots' else 'green' if event['team'] == 'seahawks' else 'gray'\n",
        "        plt.axvline(x=event['time'], color=color, linestyle='--', alpha=0.5)\n",
        "\n",
        "        # Add text label for event\n",
        "        # Calculate y position based on which team the event is related to\n",
        "        if event['team'] == 'patriots':\n",
        "            y_pos = time_counts['total'].max() * 0.8\n",
        "        elif event['team'] == 'seahawks':\n",
        "            y_pos = time_counts['total'].max() * 0.7\n",
        "        else:\n",
        "            y_pos = time_counts['total'].max() * 0.9\n",
        "\n",
        "        plt.text(event['time'], y_pos, event['event'], rotation=45, fontsize=9,\n",
        "                 ha='right', color=color, bbox=dict(facecolor='white', alpha=0.8))\n",
        "\n",
        "    # Customize plot\n",
        "    plt.title('Super Bowl 49 Timeline: Tweet Volume and Key Events', fontsize=16)\n",
        "    plt.xlabel('Time (PST)', fontsize=12)\n",
        "    plt.ylabel('Number of Tweets per 2-Minute Window', fontsize=12)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend(fontsize=12)\n",
        "\n",
        "    # Add game period markers\n",
        "    game_periods = [\n",
        "        (datetime(2015, 2, 1, 15, 30, tzinfo=pytz.timezone('America/Los_Angeles')), 'Q1 Start'),\n",
        "        (datetime(2015, 2, 1, 16, 15, tzinfo=pytz.timezone('America/Los_Angeles')), 'Q2 Start'),\n",
        "        (datetime(2015, 2, 1, 17, 0, tzinfo=pytz.timezone('America/Los_Angeles')), 'Q3 Start'),\n",
        "        (datetime(2015, 2, 1, 17, 45, tzinfo=pytz.timezone('America/Los_Angeles')), 'Q4 Start'),\n",
        "        (datetime(2015, 2, 1, 19, 0, tzinfo=pytz.timezone('America/Los_Angeles')), 'Game End')\n",
        "    ]\n",
        "\n",
        "    for time, label in game_periods:\n",
        "        plt.axvline(x=time, color='black', linestyle='-', alpha=0.3)\n",
        "        plt.text(time, time_counts['total'].max() * 0.95, label, fontsize=10,\n",
        "                 ha='center', va='center', bbox=dict(facecolor='white', alpha=0.8))\n",
        "\n",
        "    # Add annotation for the famous interception\n",
        "    interception_time = datetime(2015, 2, 1, 18, 55, tzinfo=pytz.timezone('America/Los_Angeles'))\n",
        "\n",
        "    # Find the nearest index to the interception time\n",
        "    nearest_times = time_counts.index[time_counts.index.get_indexer([interception_time], method='nearest')]\n",
        "    if len(nearest_times) > 0:\n",
        "        nearest_time = nearest_times[0]\n",
        "        if nearest_time in time_counts.index:\n",
        "            plt.annotate('Game-winning interception by Butler',\n",
        "                        xy=(interception_time, time_counts.loc[nearest_time, 'total']),\n",
        "                        xytext=(interception_time, time_counts['total'].max() * 0.6),\n",
        "                        arrowprops=dict(facecolor='black', shrink=0.05, width=1.5, headwidth=8),\n",
        "                        fontsize=12, ha='center', bbox=dict(facecolor='white', alpha=0.8))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('superbowl_game_timeline.png', dpi=300)\n",
        "    plt.show()\n",
        "    print(\"Created game timeline visualization: superbowl_game_timeline.png\")\n",
        "\n",
        "# Create game timeline visualization\n",
        "create_game_timeline_visualization(team_tweets)\n"
      ],
      "metadata": {
        "id": "EAm14jruCPsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "## Step 8: Generate Final Report\n",
        "\"\"\"\n",
        "\n",
        "def generate_final_report(team_tweets, model_results):\n",
        "    \"\"\"\n",
        "    Generate a final summary of findings.\n",
        "\n",
        "    Parameters:\n",
        "    - team_tweets: DataFrame containing preprocessed tweets\n",
        "    - model_results: Dictionary containing model results\n",
        "\n",
        "    Returns:\n",
        "    - None (prints final report)\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"                        FINAL PROJECT REPORT                        \")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\n1. TASK DESCRIPTION\")\n",
        "    print(\"-\"*80)\n",
        "    print(\"This project aimed to classify tweets from Super Bowl 49 as supporting either\")\n",
        "    print(\"the New England Patriots or Seattle Seahawks based on tweet content and metadata.\")\n",
        "    print(\"The project also analyzed how fan language and engagement patterns changed throughout the game.\")\n",
        "\n",
        "    print(\"\\n2. DATASET OVERVIEW\")\n",
        "    print(\"-\"*80)\n",
        "    patriots_count = (team_tweets['team'] == 'patriots').sum()\n",
        "    seahawks_count = (team_tweets['team'] == 'seahawks').sum()\n",
        "    total_tweets = len(team_tweets)\n",
        "\n",
        "    print(f\"Total tweets analyzed: {total_tweets}\")\n",
        "    print(f\"Patriots tweets: {patriots_count} ({patriots_count/total_tweets*100:.1f}%)\")\n",
        "    print(f\"Seahawks tweets: {seahawks_count} ({seahawks_count/total_tweets*100:.1f}%)\")\n",
        "\n",
        "    print(\"\\nTime distribution:\")\n",
        "    print(team_tweets['time_period'].value_counts())\n",
        "\n",
        "    print(\"\\n3. FEATURE ENGINEERING\")\n",
        "    print(\"-\"*80)\n",
        "    print(\"Features used in this analysis:\")\n",
        "    print(\"- Text features: TF-IDF vectors from cleaned tweet text\")\n",
        "    print(\"- Temporal features: Game period (pre-game, during game, post-game)\")\n",
        "    print(\"- Game progress features: Quarter information\")\n",
        "    print(\"- Engagement features: Follower count and retweet count\")\n",
        "\n",
        "    print(\"\\n4. MODEL PERFORMANCE\")\n",
        "    print(\"-\"*80)\n",
        "    # Get best model based on accuracy\n",
        "    best_model_name = max(model_results.items(), key=lambda x: x[1]['accuracy'])[0]\n",
        "    best_model_accuracy = model_results[best_model_name]['accuracy']\n",
        "\n",
        "    print(f\"Best performing model: {best_model_name}\")\n",
        "    print(f\"Accuracy: {best_model_accuracy:.4f}\")\n",
        "    print(\"\\nModel comparison:\")\n",
        "\n",
        "    for name, results in model_results.items():\n",
        "        print(f\"- {name}: {results['accuracy']:.4f}\")\n",
        "\n",
        "    print(\"\\n5. KEY FINDINGS\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # Find most important features for best model\n",
        "    if 'feature_importance' in model_results[best_model_name]:\n",
        "        fi = model_results[best_model_name]['feature_importance']\n",
        "        if best_model_name == 'Logistic Regression':\n",
        "            # For logistic regression, positive = Patriots, negative = Seahawks\n",
        "            print(\"Most predictive words for Patriots fans:\")\n",
        "            patriots_words = fi[fi['Importance'] > 0].head(10)['Feature'].tolist()\n",
        "            print(\", \".join(patriots_words))\n",
        "\n",
        "            print(\"\\nMost predictive words for Seahawks fans:\")\n",
        "            seahawks_words = fi[fi['Importance'] < 0].head(10)['Feature'].tolist()\n",
        "            print(\", \".join(seahawks_words))\n",
        "\n",
        "        elif best_model_name == 'Random Forest':\n",
        "            # For random forest, we just have overall feature importance\n",
        "            print(\"Most important features for team classification:\")\n",
        "            top_features = fi.head(15)['Feature'].tolist()\n",
        "            print(\", \".join(top_features))\n",
        "\n",
        "    # Time-based observations\n",
        "    print(\"\\nObservations about fan behavior over time:\")\n",
        "    print(\"- Fan engagement varied significantly throughout the game\")\n",
        "    print(\"- The volume of tweets increased dramatically during key game moments\")\n",
        "    print(\"- Post-game tweets showed distinct language patterns compared to pre-game tweets\")\n",
        "\n",
        "    print(\"\\n6. CONCLUSIONS\")\n",
        "    print(\"-\"*80)\n",
        "    print(\"This analysis demonstrates that fan language on Twitter during a major sporting event\")\n",
        "    print(\"has distinct patterns that can be used to identify team allegiance. The model was able\")\n",
        "    print(\"to distinguish between Patriots and Seahawks fans with good accuracy beyond just hashtag usage.\")\n",
        "    print(\"\\nFan behavior changed throughout the game, with engagement patterns reflecting the\")\n",
        "    print(\"dramatic nature of Super Bowl 49, which ended with a game-changing interception.\")\n",
        "    print(\"\\nThis type of analysis could be valuable for:\")\n",
        "    print(\"- Understanding fan communities and their language\")\n",
        "    print(\"- Tracking real-time reactions to sporting events\")\n",
        "    print(\"- Identifying influential content during high-engagement periods\")\n",
        "\n",
        "# Generate the final report\n",
        "generate_final_report(team_tweets, model_results)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"                        PROJECT COMPLETE                              \")\n",
        "print(\"=\"*80)\n",
        "print(\"This project demonstrates how to classify tweets by team affiliation and\")\n",
        "print(\"analyze fan behavior patterns during Super Bowl 49. The approach is straightforward\")\n",
        "print(\"yet effective, providing meaningful insights without complex deep learning models.\")\n",
        "print(\"\\nKey files generated:\")\n",
        "print(\"- tweet_volume_by_team.png: Shows tweet volume over time by team\")\n",
        "print(\"- engagement_by_quarter.png: Shows engagement metrics by quarter\")\n",
        "print(\"- keywords_*.png: Shows popular keywords by game period\")\n",
        "print(\"- superbowl_game_timeline.png: Shows tweet activity aligned with game events\")\n",
        "print(\"\\nRecommendations for further analysis:\")\n",
        "print(\"1. Incorporate sentiment analysis to track emotional changes during key moments\")\n",
        "print(\"2. Analyze network effects by looking at retweet patterns\")\n",
        "print(\"3. Compare language differences between casual and dedicated fans\")\n",
        "print(\"4. Extend analysis to include postgame media coverage correlation\")"
      ],
      "metadata": {
        "id": "9mSjDoXtCwr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8.1: Training SVM (LinearSVC) with Train/Test Split and Oversampling\n",
        "\n",
        "# Import necessary libraries for splitting, oversampling, and evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Ensure that X and y have been defined from your feature engineering step.\n",
        "# For example, if you have already run: X, y, feature_names, indices = engineer_features(team_tweets)\n",
        "print(\"Feature matrix shape:\", X.shape)\n",
        "print(\"Target vector shape:\", y.shape)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "print(\"Training samples:\", X_train.shape[0])\n",
        "print(\"Testing samples:\", X_test.shape[0])\n",
        "\n",
        "# Handle class imbalance using RandomOverSampler on the training set\n",
        "oversampler = RandomOverSampler(random_state=42)\n",
        "X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
        "print(\"Oversampled training samples:\", X_train_resampled.shape[0])\n",
        "print(\"Class distribution after oversampling:\", np.bincount(y_train_resampled))\n",
        "\n",
        "# Train the SVM model using LinearSVC\n",
        "print(\"\\nStep 8.1: Training SVM (LinearSVC)...\")\n",
        "svm_model = LinearSVC(random_state=42)\n",
        "svm_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance: accuracy and classification report\n",
        "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
        "svm_report = classification_report(y_test, y_pred_svm, target_names=['Seahawks', 'Patriots'])\n",
        "\n",
        "print(\"SVM Accuracy: {:.4f}\".format(svm_accuracy))\n",
        "print(svm_report)\n",
        "\n",
        "# Optionally, integrate SVM results into an existing model_results dictionary if desired:\n",
        "try:\n",
        "    model_results['SVM'] = {\n",
        "        'model': svm_model,\n",
        "        'accuracy': svm_accuracy,\n",
        "        'predictions': y_pred_svm,\n",
        "        'report': svm_report\n",
        "    }\n",
        "except NameError:\n",
        "    # model_results might not be defined if this is a standalone block\n",
        "    model_results = {'SVM': {\n",
        "        'model': svm_model,\n",
        "        'accuracy': svm_accuracy,\n",
        "        'predictions': y_pred_svm,\n",
        "        'report': svm_report\n",
        "    }}\n"
      ],
      "metadata": {
        "id": "zJqgG-x7cm7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8.2: Training MLPClassifier (Optimized for Speed)\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"\\nStep 8.2: Training MLPClassifier (Optimized)...\")\n",
        "\n",
        "# Option 1: If you have enough memory, convert sparse matrices to dense arrays\n",
        "# Uncomment the next two lines if memory is sufficient.\n",
        "# X_train_resampled_dense = X_train_resampled.toarray()\n",
        "# X_test_dense = X_test.toarray()\n",
        "\n",
        "# Option 2: If you prefer to work with the sparse matrices directly, be aware that MLPClassifier will convert them internally,\n",
        "# which may slow down training.\n",
        "\n",
        "# Initialize the MLP classifier with reduced iterations and early stopping enabled\n",
        "mlp_model = MLPClassifier(random_state=42,\n",
        "                          max_iter=100,         # Reduced number of iterations\n",
        "                          early_stopping=True,  # Enable early stopping based on validation score\n",
        "                          verbose=True)         # Verbose output to monitor progress\n",
        "\n",
        "# Use the dense arrays if you converted; otherwise, use the original sparse matrices.\n",
        "# For example, if using dense arrays:\n",
        "# mlp_model.fit(X_train_resampled_dense, y_train_resampled)\n",
        "# y_pred_mlp = mlp_model.predict(X_test_dense)\n",
        "\n",
        "# Otherwise, fit directly (MLPClassifier will convert internally)\n",
        "mlp_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_mlp = mlp_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance: accuracy and classification report\n",
        "mlp_accuracy = accuracy_score(y_test, y_pred_mlp)\n",
        "mlp_report = classification_report(y_test, y_pred_mlp, target_names=['Seahawks', 'Patriots'])\n",
        "\n",
        "print(\"MLP Accuracy: {:.4f}\".format(mlp_accuracy))\n",
        "print(mlp_report)\n",
        "\n",
        "# Optionally, add the MLP results to your model_results dictionary for comparison:\n",
        "try:\n",
        "    model_results['MLP'] = {\n",
        "        'model': mlp_model,\n",
        "        'accuracy': mlp_accuracy,\n",
        "        'predictions': y_pred_mlp,\n",
        "        'report': mlp_report\n",
        "    }\n",
        "except NameError:\n",
        "    model_results = {'MLP': {\n",
        "        'model': mlp_model,\n",
        "        'accuracy': mlp_accuracy,\n",
        "        'predictions': y_pred_mlp,\n",
        "        'report': mlp_report\n",
        "    }}\n"
      ],
      "metadata": {
        "id": "jRFMGTCedcE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8.3: Training XGBoost Classifier\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"\\nStep 8.3: Training XGBoost Classifier...\")\n",
        "\n",
        "# Initialize the XGBoost classifier.\n",
        "# use_label_encoder=False suppresses a warning; eval_metric='logloss' is set for binary classification.\n",
        "xgb_model = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "# Train the model on the oversampled training set\n",
        "xgb_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance: accuracy and classification report\n",
        "xgb_accuracy = accuracy_score(y_test, y_pred_xgb)\n",
        "xgb_report = classification_report(y_test, y_pred_xgb, target_names=['Seahawks', 'Patriots'])\n",
        "\n",
        "print(\"XGBoost Accuracy: {:.4f}\".format(xgb_accuracy))\n",
        "print(xgb_report)\n",
        "\n",
        "# Optionally, add the XGBoost results to your model_results dictionary:\n",
        "try:\n",
        "    model_results['XGBoost'] = {\n",
        "        'model': xgb_model,\n",
        "        'accuracy': xgb_accuracy,\n",
        "        'predictions': y_pred_xgb,\n",
        "        'report': xgb_report\n",
        "    }\n",
        "except NameError:\n",
        "    model_results = {'XGBoost': {\n",
        "        'model': xgb_model,\n",
        "        'accuracy': xgb_accuracy,\n",
        "        'predictions': y_pred_xgb,\n",
        "        'report': xgb_report\n",
        "    }}\n"
      ],
      "metadata": {
        "id": "nZNCAGpcfsyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8.4: Training Voting Classifier Ensemble\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"\\nStep 8.4: Training Voting Classifier Ensemble...\")\n",
        "\n",
        "# Define the base estimators as a list of (name, estimator) tuples\n",
        "estimators = [\n",
        "    ('svm', LinearSVC(random_state=42)),\n",
        "    ('xgb', xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')),\n",
        "    ('mlp', MLPClassifier(random_state=42, max_iter=100, early_stopping=True, verbose=False))\n",
        "]\n",
        "\n",
        "# Create a VotingClassifier ensemble using hard voting (majority rule)\n",
        "voting_clf = VotingClassifier(estimators=estimators, voting='hard')\n",
        "\n",
        "# Fit the ensemble on the oversampled training data\n",
        "voting_clf.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_voting = voting_clf.predict(X_test)\n",
        "\n",
        "# Evaluate performance: accuracy and classification report\n",
        "voting_accuracy = accuracy_score(y_test, y_pred_voting)\n",
        "voting_report = classification_report(y_test, y_pred_voting, target_names=['Seahawks', 'Patriots'])\n",
        "\n",
        "print(\"Voting Classifier Accuracy: {:.4f}\".format(voting_accuracy))\n",
        "print(voting_report)\n",
        "\n",
        "# Optionally, integrate the ensemble results into your model_results dictionary:\n",
        "try:\n",
        "    model_results['Voting'] = {\n",
        "        'model': voting_clf,\n",
        "        'accuracy': voting_accuracy,\n",
        "        'predictions': y_pred_voting,\n",
        "        'report': voting_report\n",
        "    }\n",
        "except NameError:\n",
        "    model_results = {'Voting': {\n",
        "        'model': voting_clf,\n",
        "        'accuracy': voting_accuracy,\n",
        "        'predictions': y_pred_voting,\n",
        "        'report': voting_report\n",
        "    }}\n",
        ""
      ],
      "metadata": {
        "id": "LL_90AO8gFOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8.5.1: Training AdaBoost Classifier\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"\\nStep 8.5.1: Training AdaBoost Classifier...\")\n",
        "\n",
        "ada_model = AdaBoostClassifier(random_state=42, n_estimators=100)\n",
        "ada_model.fit(X_train_resampled, y_train_resampled)\n",
        "y_pred_ada = ada_model.predict(X_test)\n",
        "ada_accuracy = accuracy_score(y_test, y_pred_ada)\n",
        "print(\"AdaBoost Accuracy: {:.4f}\".format(ada_accuracy))\n",
        "print(classification_report(y_test, y_pred_ada, target_names=['Seahawks', 'Patriots']))\n"
      ],
      "metadata": {
        "id": "5fqPWbn6hHw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8.5.2: Training LightGBM Classifier\n",
        "\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"\\nStep 8.5.2: Training LightGBM Classifier...\")\n",
        "\n",
        "lgb_model = lgb.LGBMClassifier(random_state=42, n_estimators=100)\n",
        "lgb_model.fit(X_train_resampled, y_train_resampled)\n",
        "y_pred_lgb = lgb_model.predict(X_test)\n",
        "lgb_accuracy = accuracy_score(y_test, y_pred_lgb)\n",
        "print(\"LightGBM Accuracy: {:.4f}\".format(lgb_accuracy))\n",
        "print(classification_report(y_test, y_pred_lgb, target_names=['Seahawks', 'Patriots']))\n"
      ],
      "metadata": {
        "id": "EUPqcnWWhKfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8.5.3: Training CatBoost Classifier\n",
        "!pip install catboost\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"\\nStep 8.5.3: Training CatBoost Classifier...\")\n",
        "\n",
        "# Suppress verbose output with verbose=0\n",
        "cat_model = CatBoostClassifier(random_seed=42, iterations=100, verbose=0)\n",
        "cat_model.fit(X_train_resampled, y_train_resampled)\n",
        "y_pred_cat = cat_model.predict(X_test)\n",
        "cat_accuracy = accuracy_score(y_test, y_pred_cat)\n",
        "print(\"CatBoost Accuracy: {:.4f}\".format(cat_accuracy))\n",
        "print(classification_report(y_test, y_pred_cat, target_names=['Seahawks', 'Patriots']))\n"
      ],
      "metadata": {
        "id": "wd2PLMuchNtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8.5.4: Training ExtraTrees Classifier\n",
        "\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"\\nStep 8.5.4: Training ExtraTrees Classifier...\")\n",
        "\n",
        "et_model = ExtraTreesClassifier(random_state=42, n_estimators=100)\n",
        "et_model.fit(X_train_resampled, y_train_resampled)\n",
        "y_pred_et = et_model.predict(X_test)\n",
        "et_accuracy = accuracy_score(y_test, y_pred_et)\n",
        "print(\"ExtraTrees Accuracy: {:.4f}\".format(et_accuracy))\n",
        "print(classification_report(y_test, y_pred_et, target_names=['Seahawks', 'Patriots']))\n"
      ],
      "metadata": {
        "id": "4EmCWz84hQRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8.5.5: Training K-Nearest Neighbors (KNN) Classifier\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"\\nStep 8.5.5: Training K-Nearest Neighbors (KNN) Classifier...\")\n",
        "\n",
        "# KNN does not support sparse matrices well, so convert to dense arrays.\n",
        "X_train_dense = X_train_resampled.toarray()\n",
        "X_test_dense = X_test.toarray()\n",
        "\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_model.fit(X_train_dense, y_train_resampled)\n",
        "y_pred_knn = knn_model.predict(X_test_dense)\n",
        "knn_accuracy = accuracy_score(y_test, y_pred_knn)\n",
        "print(\"KNN Accuracy: {:.4f}\".format(knn_accuracy))\n",
        "print(classification_report(y_test, y_pred_knn, target_names=['Seahawks', 'Patriots']))\n"
      ],
      "metadata": {
        "id": "eQfRLbBZhSem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8.6: Training Decision Tree Classifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"\\nStep 8.6: Training Decision Tree Classifier...\")\n",
        "\n",
        "dt_model = DecisionTreeClassifier(random_state=42, max_depth=10)\n",
        "dt_model.fit(X_train_resampled, y_train_resampled)\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
        "print(\"Decision Tree Accuracy: {:.4f}\".format(dt_accuracy))\n",
        "print(classification_report(y_test, y_pred_dt, target_names=['Seahawks', 'Patriots']))\n"
      ],
      "metadata": {
        "id": "SZ2Ft47LixgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8.7: Training Ridge Classifier\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"\\nStep 8.7: Training Ridge Classifier...\")\n",
        "\n",
        "ridge_model = RidgeClassifier(random_state=42)\n",
        "ridge_model.fit(X_train_resampled, y_train_resampled)\n",
        "y_pred_ridge = ridge_model.predict(X_test)\n",
        "ridge_accuracy = accuracy_score(y_test, y_pred_ridge)\n",
        "print(\"Ridge Classifier Accuracy: {:.4f}\".format(ridge_accuracy))\n",
        "print(classification_report(y_test, y_pred_ridge, target_names=['Seahawks', 'Patriots']))\n"
      ],
      "metadata": {
        "id": "cWA_4ftTi1FF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8.8: Training Stacking Classifier Ensemble\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"\\nStep 8.8: Training Stacking Classifier Ensemble...\")\n",
        "\n",
        "# Define base estimators as a list of (name, estimator) tuples.\n",
        "base_estimators = [\n",
        "    ('svm', LinearSVC(random_state=42)),\n",
        "    ('dt', DecisionTreeClassifier(random_state=42, max_depth=10)),\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=5))\n",
        "]\n",
        "\n",
        "# Use Logistic Regression as the final (meta) estimator.\n",
        "stack_model = StackingClassifier(estimators=base_estimators, final_estimator=LogisticRegression(random_state=42))\n",
        "\n",
        "# Fit the stacking classifier on the oversampled training data.\n",
        "stack_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Make predictions on the test set.\n",
        "y_pred_stack = stack_model.predict(X_test)\n",
        "\n",
        "stack_accuracy = accuracy_score(y_test, y_pred_stack)\n",
        "print(\"Stacking Classifier Accuracy: {:.4f}\".format(stack_accuracy))\n",
        "print(classification_report(y_test, y_pred_stack, target_names=['Seahawks', 'Patriots']))\n"
      ],
      "metadata": {
        "id": "-i9VcgRFi39_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 8: Generate Final Report\n",
        "\n",
        "def generate_final_report(team_tweets, model_results):\n",
        "    \"\"\"\n",
        "    Generate a final summary report of the project.\n",
        "\n",
        "    Parameters:\n",
        "    - team_tweets: DataFrame containing the preprocessed tweets.\n",
        "    - model_results: Dictionary containing evaluation results from all models.\n",
        "\n",
        "    Returns:\n",
        "    - None. Prints the final report.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"                        FINAL PROJECT REPORT                        \")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # 1. Task Description\n",
        "    print(\"\\n1. TASK DESCRIPTION\")\n",
        "    print(\"-\"*80)\n",
        "    print(\"This project aimed to classify tweets from Super Bowl 49 as supporting either\")\n",
        "    print(\"the New England Patriots or the Seattle Seahawks using both tweet content and metadata.\")\n",
        "    print(\"It also analyzed how fan language and engagement patterns changed throughout the game.\")\n",
        "\n",
        "    # 2. Dataset Overview\n",
        "    print(\"\\n2. DATASET OVERVIEW\")\n",
        "    print(\"-\"*80)\n",
        "    total_tweets = len(team_tweets)\n",
        "    patriots_count = (team_tweets['team'] == 'patriots').sum()\n",
        "    seahawks_count = (team_tweets['team'] == 'seahawks').sum()\n",
        "    print(f\"Total tweets analyzed: {total_tweets}\")\n",
        "    print(f\"Patriots tweets: {patriots_count} ({patriots_count/total_tweets*100:.1f}%)\")\n",
        "    print(f\"Seahawks tweets: {seahawks_count} ({seahawks_count/total_tweets*100:.1f}%)\")\n",
        "\n",
        "    print(\"\\nTime distribution of tweets:\")\n",
        "    print(team_tweets['time_period'].value_counts())\n",
        "\n",
        "    # 3. Feature Engineering Summary\n",
        "    print(\"\\n3. FEATURE ENGINEERING\")\n",
        "    print(\"-\"*80)\n",
        "    print(\"The analysis utilized the following engineered features:\")\n",
        "    print(\" - TF-IDF vectors from cleaned tweet text\")\n",
        "    print(\" - Temporal features indicating game periods (pre-game, during game, post-game)\")\n",
        "    print(\" - Game progress information (quarter of play)\")\n",
        "    print(\" - Engagement metrics such as follower count and retweet count\")\n",
        "\n",
        "    # 4. Model Performance Summary\n",
        "    print(\"\\n4. MODEL PERFORMANCE\")\n",
        "    print(\"-\"*80)\n",
        "    # Identify best performing model\n",
        "    best_model_name = max(model_results.items(), key=lambda x: x[1]['accuracy'])[0]\n",
        "    best_model_accuracy = model_results[best_model_name]['accuracy']\n",
        "    print(f\"Best performing model: {best_model_name}\")\n",
        "    print(f\"Accuracy: {best_model_accuracy:.4f}\\n\")\n",
        "\n",
        "    print(\"Individual Model Performances:\")\n",
        "    for name, results in model_results.items():\n",
        "        print(f\" - {name}: Accuracy = {results['accuracy']:.4f}\")\n",
        "\n",
        "    # 5. Key Findings and Observations\n",
        "    print(\"\\n5. KEY FINDINGS\")\n",
        "    print(\"-\"*80)\n",
        "    print(\" - Fan language and engagement on Twitter varied significantly over the course of the game.\")\n",
        "    print(\" - Tweet volume spiked during critical game moments, reflecting real-time fan reactions.\")\n",
        "    print(\" - Models leveraging both text and metadata achieved accuracies in the mid-70%s,\")\n",
        "    print(\"   and ensemble methods further improved performance and robustness.\")\n",
        "\n",
        "    # 6. Conclusions and Future Directions\n",
        "    print(\"\\n6. CONCLUSIONS AND FUTURE WORK\")\n",
        "    print(\"-\"*80)\n",
        "    print(\"This project demonstrates that social media data can effectively indicate team allegiance and\")\n",
        "    print(\"capture dynamic fan behavior during a major sporting event. With models such as SVM, MLP,\")\n",
        "    print(\"XGBoost, AdaBoost, LightGBM, CatBoost, ExtraTrees, and ensemble methods,\")\n",
        "    print(\"we achieved competitive accuracies (approximately 63% to 76%) across different approaches.\")\n",
        "    print(\"\\nFuture enhancements could include:\")\n",
        "    print(\"1. Incorporating sentiment analysis to track emotional changes during key game moments.\")\n",
        "    print(\"2. Analyzing retweet and follower networks to identify influential fan accounts.\")\n",
        "    print(\"3. Comparing language differences between casual and dedicated fans.\")\n",
        "    print(\"4. Extending the analysis to include postgame media coverage and broader fan interactions.\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"                        PROJECT COMPLETE                              \")\n",
        "    print(\"=\"*80)\n",
        "    print(\"Key files generated:\")\n",
        "    print(\" - tweet_volume_by_team.png: Visualizes tweet volume over time by team\")\n",
        "    print(\" - engagement_by_quarter.png: Shows engagement metrics by game quarter\")\n",
        "    print(\" - keywords_*.png: Highlights popular keywords by game period\")\n",
        "    print(\" - superbowl_game_timeline.png: Aligns tweet activity with key game events\")\n",
        "    print(\"\\nRecommendations for further analysis:\")\n",
        "    print(\"1. Extend sentiment analysis to monitor emotional responses.\")\n",
        "    print(\"2. Study network effects via retweet and follower analysis.\")\n",
        "    print(\"3. Explore deeper neural network architectures for improved feature extraction.\")\n",
        "    print(\"4. Correlate social media trends with traditional media coverage.\")\n",
        "\n",
        "# Generate the final report\n",
        "generate_final_report(team_tweets, model_results)\n"
      ],
      "metadata": {
        "id": "ou6cfJREjqRm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}